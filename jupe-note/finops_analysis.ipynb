{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cc5da2",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea46cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged output data for resource analysis\n",
    "df = pd.read_csv('../output/output_merged.csv')\n",
    "\n",
    "# Load output.csv for time series trend analysis (has better time granularity)\n",
    "df_trends = pd.read_csv('../output/output.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"üìä Main Dataset Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "print(f\"üìä Trends Dataset Shape: {df_trends.shape[0]:,} rows x {df_trends.shape[1]} columns\")\n",
    "print(f\"\\nüìÖ Main Date Range: {df['timeUsageStarted'].min()} to {df['timeUsageEnded'].max()}\")\n",
    "print(f\"üìÖ Trends Date Range: {df_trends['timeUsageStarted'].min()} to {df_trends['timeUsageEnded'].max()}\")\n",
    "print(f\"\\nüí∞ Total Cost (Main): ${df['computedAmount'].sum():,.2f}\")\n",
    "print(f\"üí∞ Total Cost (Trends): ${df_trends['computedAmount'].sum():,.2f}\")\n",
    "\n",
    "# Show first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944b4c0",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime for main dataset\n",
    "df['timeUsageStarted'] = pd.to_datetime(df['timeUsageStarted'])\n",
    "df['timeUsageEnded'] = pd.to_datetime(df['timeUsageEnded'])\n",
    "\n",
    "# Add date components for time series analysis\n",
    "df['date'] = df['timeUsageStarted'].dt.date\n",
    "df['year'] = df['timeUsageStarted'].dt.year\n",
    "df['month'] = df['timeUsageStarted'].dt.month\n",
    "df['month_name'] = df['timeUsageStarted'].dt.strftime('%Y-%m')\n",
    "df['day_of_week'] = df['timeUsageStarted'].dt.day_name()\n",
    "df['hour'] = df['timeUsageStarted'].dt.hour\n",
    "\n",
    "# Clean compartment paths - extract meaningful names\n",
    "df['compartment_name_clean'] = df['compartmentPath'].fillna('Unknown').str.split('/').str[-1]\n",
    "\n",
    "# Fill missing values\n",
    "df['service'] = df['service'].fillna('Unknown Service')\n",
    "df['region_from_call2'] = df['region_from_call2'].fillna(df['region'])\n",
    "df['shape_from_call2'] = df['shape_from_call2'].fillna('No Shape')\n",
    "\n",
    "# Process trends dataset for time series analysis\n",
    "df_trends['timeUsageStarted'] = pd.to_datetime(df_trends['timeUsageStarted'])\n",
    "df_trends['timeUsageEnded'] = pd.to_datetime(df_trends['timeUsageEnded'])\n",
    "df_trends['date'] = df_trends['timeUsageStarted'].dt.date\n",
    "df_trends['hour'] = df_trends['timeUsageStarted'].dt.hour\n",
    "df_trends['service'] = df_trends['service'].fillna('Unknown Service')\n",
    "df_trends['compartment_name_clean'] = df_trends['compartmentPath'].fillna('Unknown').str.split('/').str[-1]\n",
    "\n",
    "print(\"‚úÖ Main dataset cleaned and prepared\")\n",
    "print(f\"üìä Unique Services: {df['service'].nunique()}\")\n",
    "print(f\"üìä Unique Regions: {df['region_from_call2'].nunique()}\")\n",
    "print(f\"üìä Unique Compartments: {df['compartment_name_clean'].nunique()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Trends dataset prepared for time series analysis\")\n",
    "print(f\"üìä Trends - Unique Services: {df_trends['service'].nunique()}\")\n",
    "print(f\"üìä Trends - Time Granularity: Hourly data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9ab25",
   "metadata": {},
   "source": [
    "## 3. Executive Summary - Key Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eafa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key metrics\n",
    "total_cost = df['computedAmount'].sum()\n",
    "daily_avg_cost = df.groupby('date')['computedAmount'].sum().mean()\n",
    "top_service = df.groupby('service')['computedAmount'].sum().idxmax()\n",
    "top_service_cost = df.groupby('service')['computedAmount'].sum().max()\n",
    "top_region = df.groupby('region_from_call2')['computedAmount'].sum().idxmax()\n",
    "top_compartment = df.groupby('compartment_name_clean')['computedAmount'].sum().idxmax()\n",
    "\n",
    "# Display executive summary\n",
    "print(\"=\"*80)\n",
    "print(\" \"*25 + \"EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüí∞ Total Cost:                  ${total_cost:,.2f}\")\n",
    "print(f\"üìÖ Daily Average Cost:          ${daily_avg_cost:,.2f}\")\n",
    "print(f\"üìä Monthly Projected Cost:      ${daily_avg_cost * 30:,.2f}\")\n",
    "print(f\"\\nüèÜ Top Service:                 {top_service}\")\n",
    "print(f\"   Cost: ${top_service_cost:,.2f} ({top_service_cost/total_cost*100:.1f}%)\")\n",
    "print(f\"\\nüåç Top Region:                  {top_region}\")\n",
    "print(f\"üì¶ Top Compartment:             {top_compartment}\")\n",
    "print(f\"\\nüî¢ Total Resources:             {df['resourceId'].nunique():,}\")\n",
    "print(f\"üî¢ Active Compartments:         {df['compartment_name_clean'].nunique():,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883748dd",
   "metadata": {},
   "source": [
    "## 4. Cost Analysis by Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a93d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 services by cost\n",
    "service_costs = df.groupby('service')['computedAmount'].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "print(\"\\nüìä Top 10 Services by Cost:\")\n",
    "print(\"=\"*80)\n",
    "for idx, (service, cost) in enumerate(service_costs.items(), 1):\n",
    "    pct = (cost/total_cost)*100\n",
    "    print(f\"{idx:2d}. {service:40s} ${cost:>12,.2f} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(14, 6))\n",
    "service_costs.plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('Cost (USD)', fontsize=12)\n",
    "plt.ylabel('Service', fontsize=12)\n",
    "plt.title('Top 10 Services by Cost', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "for i, v in enumerate(service_costs.values):\n",
    "    plt.text(v, i, f' ${v:,.0f}', va='center', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bdbf5b",
   "metadata": {},
   "source": [
    "## 5. Cost Analysis by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b324bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional cost analysis\n",
    "region_costs = df.groupby('region_from_call2')['computedAmount'].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nüåç Cost Distribution by Region:\")\n",
    "print(\"=\"*80)\n",
    "for idx, (region, cost) in enumerate(region_costs.items(), 1):\n",
    "    pct = (cost/total_cost)*100\n",
    "    print(f\"{idx:2d}. {region:30s} ${cost:>12,.2f} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Visualization - Pie chart for top regions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pie chart\n",
    "top_regions = region_costs.head(8)\n",
    "other_cost = region_costs[8:].sum()\n",
    "if other_cost > 0:\n",
    "    plot_data = pd.concat([top_regions, pd.Series({'Others': other_cost})])\n",
    "else:\n",
    "    plot_data = top_regions\n",
    "\n",
    "ax1.pie(plot_data.values, labels=plot_data.index, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Cost Distribution by Region', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "region_costs.head(10).plot(kind='barh', ax=ax2, color='coral')\n",
    "ax2.set_xlabel('Cost (USD)', fontsize=12)\n",
    "ax2.set_ylabel('Region', fontsize=12)\n",
    "ax2.set_title('Top 10 Regions by Cost', fontsize=14, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23877df",
   "metadata": {},
   "source": [
    "## 6. Cost Analysis by Compartment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 compartments by cost\n",
    "compartment_costs = df.groupby('compartment_name_clean')['computedAmount'].sum().sort_values(ascending=False).head(15)\n",
    "\n",
    "print(\"\\nüì¶ Top 15 Compartments by Cost:\")\n",
    "print(\"=\"*80)\n",
    "for idx, (comp, cost) in enumerate(compartment_costs.items(), 1):\n",
    "    pct = (cost/total_cost)*100\n",
    "    print(f\"{idx:2d}. {comp:40s} ${cost:>12,.2f} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(14, 8))\n",
    "compartment_costs.plot(kind='barh', color='teal')\n",
    "plt.xlabel('Cost (USD)', fontsize=12)\n",
    "plt.ylabel('Compartment', fontsize=12)\n",
    "plt.title('Top 15 Compartments by Cost', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "for i, v in enumerate(compartment_costs.values):\n",
    "    plt.text(v, i, f' ${v:,.0f}', va='center', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3566d",
   "metadata": {},
   "source": [
    "## 7. Time Series Analysis - Daily Cost Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use df_trends (output.csv) for more granular time series analysis\n",
    "# Daily cost aggregation from trends data\n",
    "daily_costs = df_trends.groupby('date')['computedAmount'].sum().reset_index()\n",
    "daily_costs['date'] = pd.to_datetime(daily_costs['date'])\n",
    "daily_costs = daily_costs.sort_values('date')\n",
    "\n",
    "# Calculate moving averages\n",
    "daily_costs['MA3'] = daily_costs['computedAmount'].rolling(window=3, min_periods=1).mean()\n",
    "daily_costs['MA7'] = daily_costs['computedAmount'].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "print(\"\\nüìà Daily Cost Statistics (from output.csv):\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Average Daily Cost:    ${daily_costs['computedAmount'].mean():,.2f}\")\n",
    "print(f\"Median Daily Cost:     ${daily_costs['computedAmount'].median():,.2f}\")\n",
    "print(f\"Min Daily Cost:        ${daily_costs['computedAmount'].min():,.2f}\")\n",
    "print(f\"Max Daily Cost:        ${daily_costs['computedAmount'].max():,.2f}\")\n",
    "print(f\"Std Deviation:         ${daily_costs['computedAmount'].std():,.2f}\")\n",
    "print(f\"Total Days:            {len(daily_costs)}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(daily_costs['date'], daily_costs['computedAmount'], marker='o', label='Daily Cost', alpha=0.6, markersize=4)\n",
    "plt.plot(daily_costs['date'], daily_costs['MA3'], label='3-Day Moving Avg', linewidth=2)\n",
    "plt.plot(daily_costs['date'], daily_costs['MA7'], label='7-Day Moving Avg', linewidth=2)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Cost (USD)', fontsize=12)\n",
    "plt.title('Daily Cost Trend with Moving Averages (output.csv)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19eb143",
   "metadata": {},
   "source": [
    "## 8. Service Cost Distribution Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use df_trends (output.csv) for service cost trends over time\n",
    "# Top 5 services cost over time\n",
    "top5_services = df_trends.groupby('service')['computedAmount'].sum().nlargest(5).index\n",
    "service_daily = df_trends[df_trends['service'].isin(top5_services)].groupby(['date', 'service'])['computedAmount'].sum().reset_index()\n",
    "service_daily['date'] = pd.to_datetime(service_daily['date'])\n",
    "\n",
    "print(\"\\nüìä Top 5 Services for Time Series Analysis:\")\n",
    "for idx, service in enumerate(top5_services, 1):\n",
    "    total_cost = df_trends[df_trends['service'] == service]['computedAmount'].sum()\n",
    "    print(f\"{idx}. {service:30s} ${total_cost:>12,.2f}\")\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "for service in top5_services:\n",
    "    service_data = service_daily[service_daily['service'] == service].sort_values('date')\n",
    "    plt.plot(service_data['date'], service_data['computedAmount'], \n",
    "             marker='o', label=service, linewidth=2, alpha=0.7, markersize=4)\n",
    "\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Cost (USD)', fontsize=12)\n",
    "plt.title('Top 5 Services - Daily Cost Trends (output.csv)', fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640078a4",
   "metadata": {},
   "source": [
    "## 9. Compute Instance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd61f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter compute resources\n",
    "compute_df = df[df['service'] == 'Compute'].copy()\n",
    "\n",
    "if len(compute_df) > 0:\n",
    "    # Use skuName for shape analysis (more detailed than shape_from_call2)\n",
    "    # Fill missing skuName with shape_from_call2 as fallback\n",
    "    compute_df['shape_name'] = compute_df['skuName'].fillna(compute_df['shape_from_call2'])\n",
    "    \n",
    "    shape_costs = compute_df.groupby('shape_name')['computedAmount'].sum().sort_values(ascending=False).head(10)\n",
    "    \n",
    "    print(\"\\nüíª Compute Instance Analysis:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total Compute Cost:     ${compute_df['computedAmount'].sum():,.2f}\")\n",
    "    print(f\"Unique Instances:       {compute_df['resourceId'].nunique():,}\")\n",
    "    print(f\"Unique SKUs/Shapes:     {compute_df['shape_name'].nunique():,}\")\n",
    "    \n",
    "    # Shape category analysis (extract shape family from skuName or shape)\n",
    "    compute_df['shape_family'] = compute_df['shape_name'].str.extract(r'^(VM\\.|BM\\.)?([A-Za-z0-9]+)')[1]\n",
    "    shape_family_costs = compute_df.groupby('shape_family')['computedAmount'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nüî¢ Top 10 SKUs/Shapes by Cost:\")\n",
    "    for idx, (shape, cost) in enumerate(shape_costs.items(), 1):\n",
    "        instances = compute_df[compute_df['shape_name'] == shape]['resourceId'].nunique()\n",
    "        avg_cost = cost / instances if instances > 0 else 0\n",
    "        print(f\"{idx:2d}. {shape:50s} ${cost:>12,.2f} ({instances:>3} instances, ${avg_cost:>8,.2f}/instance)\")\n",
    "    \n",
    "    print(\"\\nüìä Cost by Shape Family:\")\n",
    "    for idx, (family, cost) in enumerate(shape_family_costs.head(10).items(), 1):\n",
    "        pct = (cost/compute_df['computedAmount'].sum())*100\n",
    "        instances = compute_df[compute_df['shape_family'] == family]['resourceId'].nunique()\n",
    "        print(f\"{idx:2d}. {family:20s} ${cost:>12,.2f} ({pct:>5.1f}%) - {instances} instances\")\n",
    "    \n",
    "    # Shape distribution by region\n",
    "    print(\"\\nüåç Top SKUs/Shapes by Region:\")\n",
    "    shape_region = compute_df.groupby(['region_from_call2', 'shape_name'])['computedAmount'].sum().reset_index()\n",
    "    shape_region = shape_region.sort_values('computedAmount', ascending=False).head(10)\n",
    "    for idx, row in shape_region.iterrows():\n",
    "        print(f\"   {row['region_from_call2']:20s} | {row['shape_name']:50s} | ${row['computedAmount']:>10,.2f}\")\n",
    "    \n",
    "    # Visualization - Multiple charts\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Top 10 shapes by cost\n",
    "    shape_costs.plot(kind='barh', ax=ax1, color='darkgreen')\n",
    "    ax1.set_xlabel('Cost (USD)', fontsize=11)\n",
    "    ax1.set_ylabel('SKU/Shape', fontsize=11)\n",
    "    ax1.set_title('Top 10 Compute SKUs/Shapes by Cost', fontsize=12, fontweight='bold')\n",
    "    ax1.invert_yaxis()\n",
    "    for i, v in enumerate(shape_costs.values):\n",
    "        ax1.text(v, i, f' ${v:,.0f}', va='center', fontsize=9)\n",
    "    \n",
    "    # 2. Shape family distribution (pie chart)\n",
    "    top_families = shape_family_costs.head(8)\n",
    "    other_cost = shape_family_costs[8:].sum()\n",
    "    if other_cost > 0:\n",
    "        plot_data = pd.concat([top_families, pd.Series({'Others': other_cost})])\n",
    "    else:\n",
    "        plot_data = top_families\n",
    "    ax2.pie(plot_data.values, labels=plot_data.index, autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Cost Distribution by Shape Family', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 3. Instance count by shape\n",
    "    shape_counts = compute_df.groupby('shape_name')['resourceId'].nunique().sort_values(ascending=False).head(10)\n",
    "    shape_counts.plot(kind='barh', ax=ax3, color='steelblue')\n",
    "    ax3.set_xlabel('Number of Instances', fontsize=11)\n",
    "    ax3.set_ylabel('SKU/Shape', fontsize=11)\n",
    "    ax3.set_title('Top 10 SKUs/Shapes by Instance Count', fontsize=12, fontweight='bold')\n",
    "    ax3.invert_yaxis()\n",
    "    for i, v in enumerate(shape_counts.values):\n",
    "        ax3.text(v, i, f' {v}', va='center', fontsize=9)\n",
    "    \n",
    "    # 4. Average cost per instance by shape\n",
    "    shape_avg_cost = compute_df.groupby('shape_name').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    })\n",
    "    shape_avg_cost['avg_cost'] = shape_avg_cost['computedAmount'] / shape_avg_cost['resourceId']\n",
    "    shape_avg_cost = shape_avg_cost.sort_values('avg_cost', ascending=False).head(10)\n",
    "    \n",
    "    shape_avg_cost['avg_cost'].plot(kind='barh', ax=ax4, color='coral')\n",
    "    ax4.set_xlabel('Average Cost per Instance (USD)', fontsize=11)\n",
    "    ax4.set_ylabel('SKU/Shape', fontsize=11)\n",
    "    ax4.set_title('Top 10 Most Expensive SKUs/Shapes (Avg per Instance)', fontsize=12, fontweight='bold')\n",
    "    ax4.invert_yaxis()\n",
    "    for i, v in enumerate(shape_avg_cost['avg_cost'].values):\n",
    "        ax4.text(v, i, f' ${v:,.0f}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Shape efficiency analysis\n",
    "    print(\"\\n‚ö° SKU/Shape Efficiency Analysis:\")\n",
    "    print(\"=\"*80)\n",
    "    shape_efficiency = compute_df.groupby('shape_name').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    }).reset_index()\n",
    "    shape_efficiency['cost_per_unit'] = shape_efficiency['computedAmount'] / shape_efficiency['computedQuantity'].replace(0, 1)\n",
    "    shape_efficiency = shape_efficiency.sort_values('computedAmount', ascending=False).head(10)\n",
    "    \n",
    "    print(\"\\nTop 10 SKUs/Shapes - Cost vs Usage Metrics:\")\n",
    "    print(f\"{'SKU/Shape':<50} {'Total Cost':>12} {'Total Hours':>12} {'$/Hour':>10} {'Instances':>10}\")\n",
    "    print(\"-\"*100)\n",
    "    for _, row in shape_efficiency.iterrows():\n",
    "        print(f\"{row['shape_name']:<50} ${row['computedAmount']:>11,.2f} {row['computedQuantity']:>12,.1f} ${row['cost_per_unit']:>9,.2f} {row['resourceId']:>10}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No compute resources found in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbacb43a",
   "metadata": {},
   "source": [
    "## 9a. Top Resource Consumers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76adbe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 resource consumers by cost\n",
    "resource_costs = df.groupby(['resourceName', 'service', 'region_from_call2']).agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'computedQuantity': 'sum',\n",
    "    'resourceId': 'first',\n",
    "    'compartment_name_clean': 'first'\n",
    "}).reset_index()\n",
    "resource_costs = resource_costs.sort_values('computedAmount', ascending=False).head(20)\n",
    "\n",
    "print(\"\\nüèÜ Top 20 Resource Consumers by Cost:\")\n",
    "print(\"=\"*120)\n",
    "print(f\"{'Rank':<5} {'Resource Name':<35} {'Service':<20} {'Region':<15} {'Cost':>12} {'Compartment':<25}\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "for idx, row in resource_costs.iterrows():\n",
    "    rank = resource_costs.index.get_loc(idx) + 1\n",
    "    resource_name = str(row['resourceName'])[:33] if pd.notna(row['resourceName']) else 'N/A'\n",
    "    service = str(row['service'])[:18]\n",
    "    region = str(row['region_from_call2'])[:13]\n",
    "    cost = row['computedAmount']\n",
    "    compartment = str(row['compartment_name_clean'])[:23]\n",
    "    \n",
    "    print(f\"{rank:<5} {resource_name:<35} {service:<20} {region:<15} ${cost:>11,.2f} {compartment:<25}\")\n",
    "\n",
    "# Calculate statistics\n",
    "total_top20_cost = resource_costs['computedAmount'].sum()\n",
    "pct_of_total = (total_top20_cost / df['computedAmount'].sum()) * 100\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"Total cost of Top 20 resources:     ${total_top20_cost:,.2f}\")\n",
    "print(f\"Percentage of total cost:           {pct_of_total:.1f}%\")\n",
    "print(f\"Average cost per resource (Top 20): ${resource_costs['computedAmount'].mean():,.2f}\")\n",
    "\n",
    "# Visualization - Top 20 resources\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Bar chart of top 20\n",
    "resource_names_short = resource_costs['resourceName'].apply(lambda x: str(x)[:30] + '...' if pd.notna(x) and len(str(x)) > 30 else str(x))\n",
    "ax1.barh(range(len(resource_costs)), resource_costs['computedAmount'], color='darkblue')\n",
    "ax1.set_yticks(range(len(resource_costs)))\n",
    "ax1.set_yticklabels(resource_names_short, fontsize=9)\n",
    "ax1.set_xlabel('Cost (USD)', fontsize=12)\n",
    "ax1.set_ylabel('Resource Name', fontsize=12)\n",
    "ax1.set_title('Top 20 Resource Consumers by Cost', fontsize=13, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "for i, v in enumerate(resource_costs['computedAmount'].values):\n",
    "    ax1.text(v, i, f' ${v:,.0f}', va='center', fontsize=8)\n",
    "\n",
    "# Service distribution for top 20\n",
    "service_dist = resource_costs.groupby('service')['computedAmount'].sum().sort_values(ascending=False)\n",
    "colors = plt.cm.Set3(range(len(service_dist)))\n",
    "ax2.pie(service_dist.values, labels=service_dist.index, autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "ax2.set_title('Service Distribution of Top 20 Resources', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional breakdown by service\n",
    "print(\"\\nüì¶ Top 20 Resources - Breakdown by Service:\")\n",
    "service_breakdown = resource_costs.groupby('service').agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'resourceName': 'count'\n",
    "}).sort_values('computedAmount', ascending=False)\n",
    "service_breakdown.columns = ['Total_Cost', 'Resource_Count']\n",
    "\n",
    "for service, row in service_breakdown.iterrows():\n",
    "    print(f\"  {service:25s} {row['Resource_Count']:>2} resources | ${row['Total_Cost']:>12,.2f}\")\n",
    "\n",
    "# Regional distribution\n",
    "print(\"\\nüåç Top 20 Resources - Regional Distribution:\")\n",
    "region_breakdown = resource_costs.groupby('region_from_call2').agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'resourceName': 'count'\n",
    "}).sort_values('computedAmount', ascending=False)\n",
    "region_breakdown.columns = ['Total_Cost', 'Resource_Count']\n",
    "\n",
    "for region, row in region_breakdown.iterrows():\n",
    "    print(f\"  {region:25s} {row['Resource_Count']:>2} resources | ${row['Total_Cost']:>12,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c7164",
   "metadata": {},
   "source": [
    "## 10. Storage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11314aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter storage services\n",
    "storage_services = ['Block Storage', 'Object Storage', 'File Storage', 'Archive Storage']\n",
    "storage_df = df[df['service'].isin(storage_services)].copy()\n",
    "\n",
    "if len(storage_df) > 0:\n",
    "    storage_costs = storage_df.groupby('service')['computedAmount'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nüíæ Storage Analysis:\")\n",
    "    print(\"=\"*120)\n",
    "    print(f\"Total Storage Cost:     ${storage_df['computedAmount'].sum():,.2f}\")\n",
    "    print(f\"Total Storage Usage:    {storage_df['computedQuantity'].sum():,.2f} units\")\n",
    "    \n",
    "    print(\"\\nüìä Storage Costs by Type:\")\n",
    "    for idx, (service, cost) in enumerate(storage_costs.items(), 1):\n",
    "        pct = (cost/storage_df['computedAmount'].sum())*100\n",
    "        usage = storage_df[storage_df['service'] == service]['computedQuantity'].sum()\n",
    "        print(f\"{idx}. {service:30s} ${cost:>12,.2f} ({pct:>5.1f}%) | Usage: {usage:>12,.2f}\")\n",
    "    \n",
    "    # Storage by compartment with full path\n",
    "    print(\"\\nüì¶ Storage Costs by Compartment (with Full Path):\")\n",
    "    storage_compartment = storage_df.groupby(['compartment_name_clean', 'compartmentPath']).agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    }).reset_index().sort_values('computedAmount', ascending=False).head(15)\n",
    "    \n",
    "    print(f\"{'Rank':<5} {'Compartment':<30} {'Cost':>12} {'Usage':>12} {'Resources':>10}\")\n",
    "    print(f\"{'':5} {'Full Path':<100}\")\n",
    "    print(\"-\"*120)\n",
    "    for idx, row in storage_compartment.iterrows():\n",
    "        rank = idx + 1\n",
    "        pct = (row['computedAmount']/storage_df['computedAmount'].sum())*100\n",
    "        print(f\"{rank:<5} {row['compartment_name_clean'][:28]:<30} ${row['computedAmount']:>11,.2f} {row['computedQuantity']:>12,.1f} {row['resourceId']:>10} ({pct:>4.1f}%)\")\n",
    "        print(f\"{'':5} {row['compartmentPath'][:98]}\")\n",
    "        print()\n",
    "    \n",
    "    # Storage by service and compartment (top combinations) with full path\n",
    "    print(\"\\nüîç Top 10 Storage Service-Compartment Combinations:\")\n",
    "    service_comp = storage_df.groupby(['service', 'compartment_name_clean', 'compartmentPath']).agg({\n",
    "        'computedAmount': 'sum'\n",
    "    }).reset_index().sort_values('computedAmount', ascending=False).head(10)\n",
    "    \n",
    "    print(f\"{'Rank':<5} {'Service':<20} {'Compartment':<30} {'Cost':>12}\")\n",
    "    print(f\"{'':5} {'Full Path':<100}\")\n",
    "    print(\"-\"*120)\n",
    "    for idx, row in service_comp.iterrows():\n",
    "        rank = idx + 1\n",
    "        print(f\"{rank:<5} {row['service']:<20} {row['compartment_name_clean'][:28]:<30} ${row['computedAmount']:>11,.2f}\")\n",
    "        print(f\"{'':5} {row['compartmentPath'][:98]}\")\n",
    "        print()\n",
    "    \n",
    "    # Visualization - Multiple charts\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Pie chart - Storage type distribution\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.pie(storage_costs.values, labels=storage_costs.index, autopct='%1.1f%%', startangle=90)\n",
    "    ax1.set_title('Storage Cost Distribution by Type', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # 2. Bar chart - Storage by region\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    storage_region = storage_df.groupby('region_from_call2')['computedAmount'].sum().sort_values(ascending=False).head(10)\n",
    "    storage_region.plot(kind='barh', ax=ax2, color='orange')\n",
    "    ax2.set_xlabel('Cost (USD)', fontsize=11)\n",
    "    ax2.set_ylabel('Region', fontsize=11)\n",
    "    ax2.set_title('Storage Costs by Region', fontsize=13, fontweight='bold')\n",
    "    ax2.invert_yaxis()\n",
    "    for i, v in enumerate(storage_region.values):\n",
    "        ax2.text(v, i, f' ${v:,.0f}', va='center', fontsize=9)\n",
    "    \n",
    "    # 3. Bar chart - Top 15 compartments\n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    compartment_costs = storage_compartment.set_index('compartment_name_clean')['computedAmount']\n",
    "    ax3.barh(range(len(compartment_costs)), compartment_costs.values, color='teal')\n",
    "    ax3.set_yticks(range(len(compartment_costs)))\n",
    "    ax3.set_yticklabels([str(x)[:38] for x in compartment_costs.index], fontsize=9)\n",
    "    ax3.set_xlabel('Cost (USD)', fontsize=11)\n",
    "    ax3.set_ylabel('Compartment', fontsize=11)\n",
    "    ax3.set_title('Top 15 Compartments by Storage Cost', fontsize=13, fontweight='bold')\n",
    "    ax3.invert_yaxis()\n",
    "    for i, v in enumerate(compartment_costs.values):\n",
    "        ax3.text(v, i, f' ${v:,.0f}', va='center', fontsize=9)\n",
    "    \n",
    "    # 4. Stacked bar - Storage types by compartment\n",
    "    ax4 = fig.add_subplot(gs[2, :])\n",
    "    top_comps = storage_compartment.head(10)['compartment_name_clean'].values\n",
    "    storage_pivot = storage_df[storage_df['compartment_name_clean'].isin(top_comps)].pivot_table(\n",
    "        index='compartment_name_clean', \n",
    "        columns='service', \n",
    "        values='computedAmount', \n",
    "        aggfunc='sum', \n",
    "        fill_value=0\n",
    "    )\n",
    "    storage_pivot = storage_pivot.reindex(top_comps)  # Maintain order\n",
    "    storage_pivot.plot(kind='barh', stacked=True, ax=ax4, colormap='Set3')\n",
    "    ax4.set_xlabel('Cost (USD)', fontsize=11)\n",
    "    ax4.set_ylabel('Compartment', fontsize=11)\n",
    "    ax4.set_title('Storage Type Distribution by Top 10 Compartments', fontsize=13, fontweight='bold')\n",
    "    ax4.legend(title='Storage Type', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    ax4.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics by compartment\n",
    "    print(\"\\nüìà Storage Summary by Compartment:\")\n",
    "    print(f\"Total Compartments with Storage:  {storage_df['compartment_name_clean'].nunique()}\")\n",
    "    print(f\"Average Cost per Compartment:     ${storage_df.groupby('compartment_name_clean')['computedAmount'].sum().mean():,.2f}\")\n",
    "    print(f\"Top Compartment Cost:             ${storage_compartment['computedAmount'].max():,.2f}\")\n",
    "    print(f\"Top Compartment Usage:            {storage_compartment['computedQuantity'].max():,.2f} units\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No storage resources found in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504492e6",
   "metadata": {},
   "source": [
    "## 11. Cost Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d44401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies using statistical methods\n",
    "daily_costs_sorted = daily_costs.sort_values('date').reset_index(drop=True)\n",
    "mean_cost = daily_costs_sorted['computedAmount'].mean()\n",
    "std_cost = daily_costs_sorted['computedAmount'].std()\n",
    "\n",
    "# Define anomalies as values beyond 2 standard deviations\n",
    "daily_costs_sorted['is_anomaly'] = np.abs(daily_costs_sorted['computedAmount'] - mean_cost) > (2 * std_cost)\n",
    "anomalies = daily_costs_sorted[daily_costs_sorted['is_anomaly']]\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Cost Anomaly Detection:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Threshold: ¬±2 Standard Deviations from Mean\")\n",
    "print(f\"Mean Daily Cost:      ${mean_cost:,.2f}\")\n",
    "print(f\"Standard Deviation:   ${std_cost:,.2f}\")\n",
    "print(f\"Upper Threshold:      ${mean_cost + 2*std_cost:,.2f}\")\n",
    "print(f\"Lower Threshold:      ${mean_cost - 2*std_cost:,.2f}\")\n",
    "print(f\"\\nAnomalies Detected:   {len(anomalies)}\")\n",
    "\n",
    "if len(anomalies) > 0:\n",
    "    print(\"\\nüìÖ Anomalous Days:\")\n",
    "    for _, row in anomalies.iterrows():\n",
    "        deviation = ((row['computedAmount'] - mean_cost) / mean_cost) * 100\n",
    "        print(f\"  {row['date']}: ${row['computedAmount']:,.2f} ({deviation:+.1f}% from mean)\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(daily_costs_sorted['date'], daily_costs_sorted['computedAmount'], \n",
    "             marker='o', label='Daily Cost', color='blue', alpha=0.6)\n",
    "    plt.axhline(y=mean_cost, color='green', linestyle='--', label='Mean', linewidth=2)\n",
    "    plt.axhline(y=mean_cost + 2*std_cost, color='red', linestyle='--', label='Upper Threshold', linewidth=2)\n",
    "    plt.axhline(y=mean_cost - 2*std_cost, color='red', linestyle='--', label='Lower Threshold', linewidth=2)\n",
    "    \n",
    "    # Highlight anomalies\n",
    "    if len(anomalies) > 0:\n",
    "        plt.scatter(anomalies['date'], anomalies['computedAmount'], \n",
    "                   color='red', s=200, zorder=5, label='Anomalies', marker='X')\n",
    "    \n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Cost (USD)', fontsize=12)\n",
    "    plt.title('Cost Anomaly Detection (¬±2œÉ)', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n‚úÖ No anomalies detected in the period\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd791471",
   "metadata": {},
   "source": [
    "## 12. Cost Optimization Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüí° Cost Optimization Opportunities:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Idle resources (very low usage)\n",
    "resource_usage = df.groupby('resourceId').agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'computedQuantity': 'sum',\n",
    "    'service': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Resources with cost but minimal usage\n",
    "potential_idle = resource_usage[(resource_usage['computedAmount'] > 0) & \n",
    "                                (resource_usage['computedQuantity'] < 1)]\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£ Potentially Idle Resources: {len(potential_idle)}\")\n",
    "if len(potential_idle) > 0:\n",
    "    idle_cost = potential_idle['computedAmount'].sum()\n",
    "    print(f\"   Potential Savings: ${idle_cost:,.2f}\")\n",
    "    print(\"\\n   Top 5 by Cost:\")\n",
    "    for idx, row in potential_idle.nlargest(5, 'computedAmount').iterrows():\n",
    "        print(f\"   - {row['service']:20s} ${row['computedAmount']:>10,.2f}\")\n",
    "\n",
    "# 2. Multi-region resources that could be consolidated\n",
    "multi_region_services = df.groupby('service')['region_from_call2'].nunique()\n",
    "multi_region_services = multi_region_services[multi_region_services > 3].sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£ Services Spanning Multiple Regions (3+):\")\n",
    "for service, count in multi_region_services.head(5).items():\n",
    "    service_cost = df[df['service'] == service]['computedAmount'].sum()\n",
    "    print(f\"   - {service:30s} {count} regions, ${service_cost:,.2f}\")\n",
    "\n",
    "# 3. High-cost compartments that need review\n",
    "high_cost_compartments = df.groupby('compartment_name_clean')['computedAmount'].sum().nlargest(5)\n",
    "print(f\"\\n3Ô∏è‚É£ Top 5 High-Cost Compartments (Review for optimization):\")\n",
    "for comp, cost in high_cost_compartments.items():\n",
    "    pct = (cost/total_cost)*100\n",
    "    print(f\"   - {comp:40s} ${cost:>10,.2f} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nüí° Recommendation: Review Cloud Advisor recommendations for detailed optimization guidance\")\n",
    "print(\"   Run: ./collector.sh <params> --only-recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910c8bd",
   "metadata": {},
   "source": [
    "## 13. Cost Forecast (Simple Linear Projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55475b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear forecast for next 7 days\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Prepare data\n",
    "daily_costs_forecast = daily_costs_sorted.copy()\n",
    "daily_costs_forecast['day_num'] = range(len(daily_costs_forecast))\n",
    "\n",
    "X = daily_costs_forecast[['day_num']].values\n",
    "y = daily_costs_forecast['computedAmount'].values\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Forecast next 7 days\n",
    "future_days = np.array(range(len(daily_costs_forecast), len(daily_costs_forecast) + 7)).reshape(-1, 1)\n",
    "forecast = model.predict(future_days)\n",
    "\n",
    "last_date = daily_costs_forecast['date'].max()\n",
    "forecast_dates = pd.date_range(start=last_date + timedelta(days=1), periods=7)\n",
    "\n",
    "print(\"\\nüìä 7-Day Cost Forecast (Linear Projection):\")\n",
    "print(\"=\"*80)\n",
    "total_forecast = 0\n",
    "for date, cost in zip(forecast_dates, forecast):\n",
    "    print(f\"{date.strftime('%Y-%m-%d')}: ${cost:,.2f}\")\n",
    "    total_forecast += cost\n",
    "print(f\"\\nForecast Total (7 days): ${total_forecast:,.2f}\")\n",
    "print(f\"Forecast Monthly:        ${total_forecast * 30 / 7:,.2f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(daily_costs_forecast['date'], daily_costs_forecast['computedAmount'], \n",
    "         marker='o', label='Historical', color='blue', linewidth=2)\n",
    "plt.plot(forecast_dates, forecast, marker='s', label='Forecast', \n",
    "         color='red', linestyle='--', linewidth=2)\n",
    "plt.axvline(x=last_date, color='gray', linestyle=':', linewidth=2, label='Today')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Cost (USD)', fontsize=12)\n",
    "plt.title('7-Day Cost Forecast', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7644e23",
   "metadata": {},
   "source": [
    "## 14. Export Key Insights to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export top spenders by different dimensions\n",
    "print(\"\\nüìÅ Exporting Key Insights:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Top services\n",
    "top_services_df = df.groupby('service')['computedAmount'].sum().reset_index()\n",
    "top_services_df.columns = ['Service', 'Total_Cost']\n",
    "top_services_df = top_services_df.sort_values('Total_Cost', ascending=False)\n",
    "top_services_df.to_csv('../output/analysis_top_services.csv', index=False)\n",
    "print(f\"‚úÖ Exported: ../output/analysis_top_services.csv ({len(top_services_df)} services)\")\n",
    "\n",
    "# Top compartments\n",
    "top_compartments_df = df.groupby(['compartment_name_clean', 'compartmentPath'])['computedAmount'].sum().reset_index()\n",
    "top_compartments_df.columns = ['Compartment', 'Full_Path', 'Total_Cost']\n",
    "top_compartments_df = top_compartments_df.sort_values('Total_Cost', ascending=False)\n",
    "top_compartments_df.to_csv('../output/analysis_top_compartments.csv', index=False)\n",
    "print(f\"‚úÖ Exported: ../output/analysis_top_compartments.csv ({len(top_compartments_df)} compartments)\")\n",
    "\n",
    "# Daily costs\n",
    "daily_costs_export = daily_costs_sorted[['date', 'computedAmount', 'MA3', 'MA7']]\n",
    "daily_costs_export.columns = ['Date', 'Daily_Cost', 'MA_3Day', 'MA_7Day']\n",
    "daily_costs_export.to_csv('../output/analysis_daily_costs.csv', index=False)\n",
    "print(f\"‚úÖ Exported: ../output/analysis_daily_costs.csv ({len(daily_costs_export)} days)\")\n",
    "\n",
    "# Resource-level analysis\n",
    "resource_summary = df.groupby(['resourceId', 'service', 'region_from_call2']).agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'computedQuantity': 'sum',\n",
    "    'compartment_name_clean': 'first'\n",
    "}).reset_index()\n",
    "resource_summary.columns = ['Resource_ID', 'Service', 'Region', 'Total_Cost', 'Total_Quantity', 'Compartment']\n",
    "resource_summary = resource_summary.sort_values('Total_Cost', ascending=False)\n",
    "resource_summary.to_csv('../output/analysis_resource_summary.csv', index=False)\n",
    "print(f\"‚úÖ Exported: ../output/analysis_resource_summary.csv ({len(resource_summary)} resources)\")\n",
    "\n",
    "print(\"\\n‚úÖ All insights exported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49559d95",
   "metadata": {},
   "source": [
    "## 15. Custom Query Section\n",
    "\n",
    "Use this section to run custom queries on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33730aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Filter by specific service\n",
    "# service_name = 'Compute'\n",
    "# service_data = df[df['service'] == service_name]\n",
    "# print(f\"Total {service_name} cost: ${service_data['computedAmount'].sum():,.2f}\")\n",
    "\n",
    "# Example: Filter by date range\n",
    "# start_date = '2025-11-01'\n",
    "# end_date = '2025-11-10'\n",
    "# date_filtered = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "# print(f\"Cost in date range: ${date_filtered['computedAmount'].sum():,.2f}\")\n",
    "\n",
    "# Add your custom queries here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b060856b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive FinOps intelligence including:\n",
    "- ‚úÖ Executive summary with key metrics\n",
    "- ‚úÖ Cost breakdowns by service, region, and compartment\n",
    "- ‚úÖ Time series analysis and trends\n",
    "- ‚úÖ Compute and storage deep dives\n",
    "- ‚úÖ Anomaly detection\n",
    "- ‚úÖ Cost optimization opportunities\n",
    "- ‚úÖ 7-day cost forecast\n",
    "- ‚úÖ Exportable insights for reporting\n",
    "\n",
    "### Next Steps:\n",
    "1. Review anomalies and investigate root causes\n",
    "2. Analyze high-cost compartments and services\n",
    "3. Check Cloud Advisor recommendations: `./collector.sh <params> --only-recommendations`\n",
    "4. Implement cost optimization strategies\n",
    "5. Set up regular monitoring and reporting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
