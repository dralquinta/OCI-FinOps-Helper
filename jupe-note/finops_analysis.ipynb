{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cc5da2",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea46cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged output data for resource analysis\n",
    "file = '../output/1.csv'\n",
    "\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# Load output.csv for time series trend analysis (has better time granularity)\n",
    "df_trends = pd.read_csv(file)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"üìä Main Dataset Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "print(f\"üìä Trends Dataset Shape: {df_trends.shape[0]:,} rows x {df_trends.shape[1]} columns\")\n",
    "print(f\"\\nüìÖ Main Date Range: {df['timeUsageStarted'].min()} to {df['timeUsageEnded'].max()}\")\n",
    "print(f\"üìÖ Trends Date Range: {df_trends['timeUsageStarted'].min()} to {df_trends['timeUsageEnded'].max()}\")\n",
    "print(f\"\\nüí∞ Total Cost (Main): ${df['computedAmount'].sum():,.2f}\")\n",
    "print(f\"üí∞ Total Cost (Trends): ${df_trends['computedAmount'].sum():,.2f}\")\n",
    "\n",
    "# Show first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944b4c0",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime for main dataset\n",
    "df['timeUsageStarted'] = pd.to_datetime(df['timeUsageStarted'])\n",
    "df['timeUsageEnded'] = pd.to_datetime(df['timeUsageEnded'])\n",
    "\n",
    "# Add date components for time series analysis\n",
    "df['date'] = df['timeUsageStarted'].dt.date\n",
    "df['year'] = df['timeUsageStarted'].dt.year\n",
    "df['month'] = df['timeUsageStarted'].dt.month\n",
    "df['month_name'] = df['timeUsageStarted'].dt.strftime('%Y-%m')\n",
    "df['day_of_week'] = df['timeUsageStarted'].dt.day_name()\n",
    "df['hour'] = df['timeUsageStarted'].dt.hour\n",
    "\n",
    "# Clean compartment paths - extract meaningful names\n",
    "df['compartment_name_clean'] = df['compartmentPath'].fillna('Unknown').str.split('/').str[-1]\n",
    "\n",
    "# Fill missing values\n",
    "df['service'] = df['service'].fillna('Unknown Service')\n",
    "df['region_from_call2'] = df['region_from_call2'].fillna(df['region'])\n",
    "df['shape_from_call2'] = df['shape_from_call2'].fillna('No Shape')\n",
    "\n",
    "# Process trends dataset for time series analysis\n",
    "df_trends['timeUsageStarted'] = pd.to_datetime(df_trends['timeUsageStarted'])\n",
    "df_trends['timeUsageEnded'] = pd.to_datetime(df_trends['timeUsageEnded'])\n",
    "df_trends['date'] = df_trends['timeUsageStarted'].dt.date\n",
    "df_trends['hour'] = df_trends['timeUsageStarted'].dt.hour\n",
    "df_trends['service'] = df_trends['service'].fillna('Unknown Service')\n",
    "df_trends['compartment_name_clean'] = df_trends['compartmentPath'].fillna('Unknown').str.split('/').str[-1]\n",
    "\n",
    "print(\"‚úÖ Main dataset cleaned and prepared\")\n",
    "print(f\"üìä Unique Services: {df['service'].nunique()}\")\n",
    "print(f\"üìä Unique Regions: {df['region_from_call2'].nunique()}\")\n",
    "print(f\"üìä Unique Compartments: {df['compartment_name_clean'].nunique()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Trends dataset prepared for time series analysis\")\n",
    "print(f\"üìä Trends - Unique Services: {df_trends['service'].nunique()}\")\n",
    "print(f\"üìä Trends - Time Granularity: Hourly data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de2ddc",
   "metadata": {},
   "source": [
    "## 3. Exadata Infrastructure Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74faebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Exadata Cost Analysis\n",
    "# Filter all Exadata-related services and SKUs\n",
    "exadata_keywords = ['Exadata', 'ExaCS', 'ExaCC', 'Exadata Cloud Service', 'Exadata Cloud at Customer']\n",
    "\n",
    "# Build filter conditions safely\n",
    "filter_conditions = df['service'].str.contains('Exadata', case=False, na=False)\n",
    "\n",
    "# Add skuName filter if column exists\n",
    "if 'skuName' in df.columns:\n",
    "    filter_conditions = filter_conditions | df['skuName'].fillna('').astype(str).str.contains('Exadata', case=False, na=False)\n",
    "\n",
    "# Add skuPartNumber filter if column exists\n",
    "if 'skuPartNumber' in df.columns:\n",
    "    filter_conditions = filter_conditions | df['skuPartNumber'].fillna('').astype(str).str.contains('Exadata', case=False, na=False)\n",
    "\n",
    "# Add resourceName filter for additional coverage\n",
    "if 'resourceName' in df.columns:\n",
    "    filter_conditions = filter_conditions | df['resourceName'].fillna('').astype(str).str.contains('Exadata', case=False, na=False)\n",
    "\n",
    "exadata_df = df[filter_conditions].copy()\n",
    "\n",
    "if len(exadata_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*140)\n",
    "    print(\" \"*50 + \"üè¢ EXADATA INFRASTRUCTURE ANALYSIS\")\n",
    "    print(\"=\"*140)\n",
    "    \n",
    "    # SECTION 1: EXECUTIVE SUMMARY\n",
    "    total_exadata_cost = exadata_df['computedAmount'].sum()\n",
    "    total_exadata_hours = exadata_df['computedQuantity'].sum()\n",
    "    unique_systems = exadata_df['resourceId'].nunique()\n",
    "    avg_hourly_rate = total_exadata_cost / total_exadata_hours if total_exadata_hours > 0 else 0\n",
    "    \n",
    "    print(\"\\nüìä EXECUTIVE SUMMARY:\")\n",
    "    print(\"-\" * 140)\n",
    "    print(f\"{'Total Exadata Spend:':<40} ${total_exadata_cost:>18,.2f}\")\n",
    "    print(f\"{'Percentage of Total Cloud Spend:':<40} {(total_exadata_cost/df['computedAmount'].sum())*100:>17.2f}%\")\n",
    "    print(f\"{'Total Compute Hours:':<40} {total_exadata_hours:>18,.2f}\")\n",
    "    print(f\"{'Average Hourly Rate:':<40} ${avg_hourly_rate:>18,.2f}\")\n",
    "    print(f\"{'Unique Exadata Systems/Resources:':<40} {unique_systems:>18,}\")\n",
    "    print(f\"{'Unique Compartments:':<40} {exadata_df['compartment_name_clean'].nunique():>18,}\")\n",
    "    print(f\"{'Regions with Exadata:':<40} {exadata_df['region_from_call2'].nunique():>18,}\")\n",
    "    print(f\"{'Date Range:':<40} {str(exadata_df['timeUsageStarted'].min())[:10]} to {str(exadata_df['timeUsageEnded'].max())[:10]:>6}\")\n",
    "    \n",
    "    # SECTION 2: SERVICE TYPE BREAKDOWN\n",
    "    print(\"\\n\\nüíº EXADATA SERVICE TYPE BREAKDOWN:\")\n",
    "    print(\"-\" * 140)\n",
    "    service_breakdown = exadata_df.groupby('service').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    }).sort_values('computedAmount', ascending=False)\n",
    "    \n",
    "    print(f\"{'Service Type':<50} {'Total Cost':>15} {'% of Total':>10} {'Hours':>15} {'Systems':>8}\")\n",
    "    print(\"-\" * 140)\n",
    "    for service, row in service_breakdown.iterrows():\n",
    "        pct = (row['computedAmount'] / total_exadata_cost) * 100\n",
    "        print(f\"{service:<50} ${row['computedAmount']:>14,.2f} {pct:>9.1f}% {row['computedQuantity']:>15,.1f} {row['resourceId']:>8}\")\n",
    "    \n",
    "    # SECTION 3: SKU/SHAPE ANALYSIS\n",
    "    print(\"\\n\\nüîß DETAILED SKU/SHAPE ANALYSIS:\")\n",
    "    print(\"-\" * 140)\n",
    "    \n",
    "    exadata_df['sku_clean'] = exadata_df['skuName'].fillna(exadata_df['skuPartNumber']).fillna('Unknown SKU')\n",
    "    \n",
    "    sku_analysis = exadata_df.groupby('sku_clean').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum',\n",
    "        'resourceId': 'nunique',\n",
    "        'service': 'first'\n",
    "    }).sort_values('computedAmount', ascending=False)\n",
    "    \n",
    "    sku_analysis['cost_per_hour'] = sku_analysis['computedAmount'] / sku_analysis['computedQuantity'].replace(0, 1)\n",
    "    sku_analysis['cost_per_system'] = sku_analysis['computedAmount'] / sku_analysis['resourceId']\n",
    "    \n",
    "    print(f\"{'SKU/Configuration':<60} {'Total Cost':>15} {'% Total':>8} {'Systems':>8} {'$/Hour':>12} {'$/System':>15}\")\n",
    "    print(\"-\" * 140)\n",
    "    for sku, row in sku_analysis.head(20).iterrows():\n",
    "        pct = (row['computedAmount'] / total_exadata_cost) * 100\n",
    "        sku_display = str(sku)[:58]\n",
    "        print(f\"{sku_display:<60} ${row['computedAmount']:>14,.2f} {pct:>7.1f}% {row['resourceId']:>8} ${row['cost_per_hour']:>11,.2f} ${row['cost_per_system']:>14,.2f}\")\n",
    "    \n",
    "    # SECTION 4: SHAPE/SIZE ANALYSIS\n",
    "    print(\"\\n\\nüìè EXADATA SHAPE/SIZE ANALYSIS:\")\n",
    "    print(\"-\" * 140)\n",
    "    \n",
    "    exadata_df['shape_category'] = 'Unknown'\n",
    "    exadata_df.loc[exadata_df['sku_clean'].str.contains('Quarter Rack|1/4 Rack', case=False, na=False), 'shape_category'] = 'Quarter Rack (1/4)'\n",
    "    exadata_df.loc[exadata_df['sku_clean'].str.contains('Half Rack|1/2 Rack', case=False, na=False), 'shape_category'] = 'Half Rack (1/2)'\n",
    "    exadata_df.loc[exadata_df['sku_clean'].str.contains('Full Rack', case=False, na=False), 'shape_category'] = 'Full Rack'\n",
    "    exadata_df.loc[exadata_df['sku_clean'].str.contains('Base System|X8M|X9M|X10M', case=False, na=False), 'shape_category'] = 'Base System'\n",
    "    exadata_df.loc[exadata_df['sku_clean'].str.contains('Elastic', case=False, na=False), 'shape_category'] = 'Elastic Configuration'\n",
    "    \n",
    "    shape_analysis = exadata_df.groupby('shape_category').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'resourceId': 'nunique',\n",
    "        'computedQuantity': 'sum'\n",
    "    }).sort_values('computedAmount', ascending=False)\n",
    "    \n",
    "    print(f\"{'Configuration Size':<40} {'Total Cost':>15} {'% of Total':>12} {'Systems':>10} {'Total Hours':>15}\")\n",
    "    print(\"-\" * 140)\n",
    "    for shape, row in shape_analysis.iterrows():\n",
    "        pct = (row['computedAmount'] / total_exadata_cost) * 100\n",
    "        print(f\"{shape:<40} ${row['computedAmount']:>14,.2f} {pct:>11.1f}% {row['resourceId']:>10} {row['computedQuantity']:>15,.1f}\")\n",
    "    \n",
    "    # SECTION 5: REGIONAL DISTRIBUTION\n",
    "    print(\"\\n\\nüåç REGIONAL DISTRIBUTION:\")\n",
    "    print(\"-\" * 140)\n",
    "    \n",
    "    region_analysis = exadata_df.groupby('region_from_call2').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'resourceId': 'nunique',\n",
    "        'compartment_name_clean': 'nunique'\n",
    "    }).sort_values('computedAmount', ascending=False)\n",
    "    \n",
    "    print(f\"{'Region':<35} {'Total Cost':>15} {'% of Total':>12} {'Systems':>10} {'Compartments':>15}\")\n",
    "    print(\"-\" * 140)\n",
    "    for region, row in region_analysis.iterrows():\n",
    "        pct = (row['computedAmount'] / total_exadata_cost) * 100\n",
    "        print(f\"{region:<35} ${row['computedAmount']:>14,.2f} {pct:>11.1f}% {row['resourceId']:>10} {row['compartment_name_clean']:>15}\")\n",
    "    \n",
    "    # SECTION 6: COMPARTMENT ANALYSIS\n",
    "    print(\"\\n\\nüì¶ COMPARTMENT ANALYSIS (Top 15):\")\n",
    "    print(\"-\" * 140)\n",
    "    \n",
    "    compartment_analysis = exadata_df.groupby(['compartment_name_clean', 'compartmentPath']).agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'resourceId': 'nunique',\n",
    "        'computedQuantity': 'sum'\n",
    "    }).sort_values('computedAmount', ascending=False).head(15)\n",
    "    \n",
    "    print(f\"{'Rank':<6} {'Compartment':<40} {'Total Cost':>15} {'% Total':>10} {'Systems':>8}\")\n",
    "    print(f\"{'':6} {'Full Path':<110}\")\n",
    "    print(\"-\" * 140)\n",
    "    for idx, ((comp_name, comp_path), row) in enumerate(compartment_analysis.iterrows(), 1):\n",
    "        pct = (row['computedAmount'] / total_exadata_cost) * 100\n",
    "        print(f\"{idx:<6} {comp_name[:38]:<40} ${row['computedAmount']:>14,.2f} {pct:>9.1f}% {row['resourceId']:>8}\")\n",
    "        print(f\"{'':6} {comp_path[:108]}\")\n",
    "        print()\n",
    "    \n",
    "    # SECTION 7: INDIVIDUAL SYSTEMS\n",
    "    print(\"\\n\\nüñ•Ô∏è  TOP 20 INDIVIDUAL EXADATA SYSTEMS BY COST:\")\n",
    "    print(\"-\" * 140)\n",
    "    \n",
    "    system_details = exadata_df.groupby(['resourceId', 'resourceName', 'service', 'region_from_call2', \n",
    "                                          'compartment_name_clean', 'sku_clean']).agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum'\n",
    "    }).reset_index().sort_values('computedAmount', ascending=False).head(20)\n",
    "    \n",
    "    system_details['hourly_rate'] = system_details['computedAmount'] / system_details['computedQuantity'].replace(0, 1)\n",
    "    \n",
    "    print(f\"{'Rank':<6} {'Resource Name':<35} {'Service':<30} {'Total Cost':>15}\")\n",
    "    print(f\"{'':6} {'Region':<25} {'Compartment':<40} {'$/Hour':>12}\")\n",
    "    print(f\"{'':6} {'SKU/Configuration':<100}\")\n",
    "    print(\"-\" * 140)\n",
    "    for idx, row in system_details.iterrows():\n",
    "        rank = system_details.index.get_loc(idx) + 1\n",
    "        resource_name = str(row['resourceName'])[:33] if pd.notna(row['resourceName']) else 'N/A'\n",
    "        service = str(row['service'])[:28]\n",
    "        print(f\"{rank:<6} {resource_name:<35} {service:<30} ${row['computedAmount']:>14,.2f}\")\n",
    "        print(f\"{'':6} {str(row['region_from_call2'])[:23]:<25} {str(row['compartment_name_clean'])[:38]:<40} ${row['hourly_rate']:>11,.2f}\")\n",
    "        print(f\"{'':6} {str(row['sku_clean'])[:98]}\")\n",
    "        print()\n",
    "    \n",
    "    # SECTION 8: LICENSING\n",
    "    print(\"\\n\\nüìú LICENSING & COMPONENT COST BREAKDOWN:\")\n",
    "    print(\"-\" * 140)\n",
    "    \n",
    "    exadata_df['cost_category'] = 'Infrastructure'\n",
    "    exadata_df.loc[exadata_df['sku_clean'].str.contains('OCPU|Processor|CPU', case=False, na=False), 'cost_category'] = 'Compute/OCPU'\n",
    "    exadata_df.loc[exadata_df['sku_clean'].str.contains('Storage|TB|Terabyte', case=False, na=False), 'cost_category'] = 'Storage'\n",
    "    exadata_df.loc[exadata_df['sku_clean'].str.contains('Database|Enterprise Edition|Standard Edition', case=False, na=False), 'cost_category'] = 'Database License'\n",
    "    exadata_df.loc[exadata_df['sku_clean'].str.contains('Backup|RMAN', case=False, na=False), 'cost_category'] = 'Backup'\n",
    "    exadata_df.loc[exadata_df['sku_clean'].str.contains('Support|Premier', case=False, na=False), 'cost_category'] = 'Support'\n",
    "    \n",
    "    category_breakdown = exadata_df.groupby('cost_category').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum'\n",
    "    }).sort_values('computedAmount', ascending=False)\n",
    "    \n",
    "    print(f\"{'Cost Category':<40} {'Total Cost':>15} {'% of Total':>12} {'Total Units':>15}\")\n",
    "    print(\"-\" * 140)\n",
    "    for category, row in category_breakdown.iterrows():\n",
    "        pct = (row['computedAmount'] / total_exadata_cost) * 100\n",
    "        print(f\"{category:<40} ${row['computedAmount']:>14,.2f} {pct:>11.1f}% {row['computedQuantity']:>15,.2f}\")\n",
    "    \n",
    "    # SECTION 9: TIME-BASED\n",
    "    print(\"\\n\\nüìÖ DAILY COST TREND ANALYSIS:\")\n",
    "    print(\"-\" * 140)\n",
    "    \n",
    "    daily_exadata = exadata_df.groupby(exadata_df['timeUsageStarted'].dt.date).agg({\n",
    "        'computedAmount': 'sum'\n",
    "    }).reset_index()\n",
    "    daily_exadata.columns = ['date', 'daily_cost']\n",
    "    \n",
    "    print(f\"{'Average Daily Cost:':<40} ${daily_exadata['daily_cost'].mean():>18,.2f}\")\n",
    "    print(f\"{'Median Daily Cost:':<40} ${daily_exadata['daily_cost'].median():>18,.2f}\")\n",
    "    print(f\"{'Minimum Daily Cost:':<40} ${daily_exadata['daily_cost'].min():>18,.2f}\")\n",
    "    print(f\"{'Maximum Daily Cost:':<40} ${daily_exadata['daily_cost'].max():>18,.2f}\")\n",
    "    print(f\"{'Standard Deviation:':<40} ${daily_exadata['daily_cost'].std():>18,.2f}\")\n",
    "    print(f\"{'Projected Monthly Cost:':<40} ${daily_exadata['daily_cost'].mean() * 30:>18,.2f}\")\n",
    "    print(f\"{'Projected Annual Cost:':<40} ${daily_exadata['daily_cost'].mean() * 365:>18,.2f}\")\n",
    "    \n",
    "    # SECTION 10: EFFICIENCY\n",
    "    print(\"\\n\\n‚ö° COST EFFICIENCY METRICS:\")\n",
    "    print(\"-\" * 140)\n",
    "    \n",
    "    efficiency_by_sku = exadata_df.groupby('sku_clean').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    })\n",
    "    efficiency_by_sku['cost_per_hour'] = efficiency_by_sku['computedAmount'] / efficiency_by_sku['computedQuantity'].replace(0, 1)\n",
    "    efficiency_by_sku['cost_per_system_per_day'] = (efficiency_by_sku['computedAmount'] / efficiency_by_sku['resourceId']) / (efficiency_by_sku['computedQuantity'] / efficiency_by_sku['resourceId'] / 24).replace(0, 1)\n",
    "    efficiency_by_sku = efficiency_by_sku.sort_values('cost_per_hour', ascending=False).head(10)\n",
    "    \n",
    "    print(f\"{'SKU (Top 10 Most Expensive)':<65} {'$/Hour':>12} {'$/System/Day':>15} {'Systems':>8}\")\n",
    "    print(\"-\" * 140)\n",
    "    for sku, row in efficiency_by_sku.iterrows():\n",
    "        sku_display = str(sku)[:63]\n",
    "        print(f\"{sku_display:<65} ${row['cost_per_hour']:>11,.2f} ${row['cost_per_system_per_day']:>14,.2f} {row['resourceId']:>8}\")\n",
    "    \n",
    "    # SECTION 11: OPTIMIZATION\n",
    "    print(\"\\n\\nüí° EXADATA COST OPTIMIZATION OPPORTUNITIES:\")\n",
    "    print(\"-\" * 140)\n",
    "    \n",
    "    system_utilization = exadata_df.groupby('resourceId').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum'\n",
    "    })\n",
    "    system_utilization['utilization_ratio'] = system_utilization['computedQuantity'] / system_utilization['computedQuantity'].max()\n",
    "    underutilized = system_utilization[system_utilization['utilization_ratio'] < 0.5].sort_values('computedAmount', ascending=False)\n",
    "    \n",
    "    if len(underutilized) > 0:\n",
    "        print(f\"\\n1Ô∏è‚É£  POTENTIALLY UNDERUTILIZED SYSTEMS: {len(underutilized)}\")\n",
    "        print(f\"    Total Cost: ${underutilized['computedAmount'].sum():,.2f}\")\n",
    "    \n",
    "    if exadata_df['region_from_call2'].nunique() > 1:\n",
    "        print(f\"\\n2Ô∏è‚É£  MULTI-REGION DEPLOYMENT:\")\n",
    "        print(f\"    Exadata deployed across {exadata_df['region_from_call2'].nunique()} regions\")\n",
    "    \n",
    "    if 'Full Rack' in exadata_df['shape_category'].values or 'Half Rack' in exadata_df['shape_category'].values:\n",
    "        large_config_cost = exadata_df[exadata_df['shape_category'].isin(['Full Rack', 'Half Rack'])]['computedAmount'].sum()\n",
    "        print(f\"\\n3Ô∏è‚É£  LARGE CONFIGURATION ANALYSIS:\")\n",
    "        print(f\"    Full/Half Rack systems cost: ${large_config_cost:,.2f}\")\n",
    "    \n",
    "    top_3_systems_cost = system_details.head(3)['computedAmount'].sum()\n",
    "    top_3_pct = (top_3_systems_cost / total_exadata_cost) * 100\n",
    "    print(f\"\\n4Ô∏è‚É£  COST CONCENTRATION:\")\n",
    "    print(f\"    Top 3 systems: {top_3_pct:.1f}% of total (${top_3_systems_cost:,.2f})\")\n",
    "    \n",
    "    print(f\"\\n5Ô∏è‚É£  RECOMMENDATIONS:\")\n",
    "    print(f\"    ‚Ä¢ Review Cloud Advisor for Exadata-specific recommendations\")\n",
    "    print(f\"    ‚Ä¢ Consider Autonomous Database migration\")\n",
    "    print(f\"    ‚Ä¢ Evaluate ExaCC vs ExaCS cost-benefit\")\n",
    "    \n",
    "    # VISUALIZATIONS\n",
    "    if total_exadata_cost > 0:\n",
    "        print(\"\\n\\nüìä Generating visualizations...\")\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 16))\n",
    "        gs = fig.add_gridspec(4, 2, hspace=0.35, wspace=0.3)\n",
    "        \n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        service_costs = service_breakdown['computedAmount']\n",
    "        if len(service_costs) > 5:\n",
    "            top_services = service_costs.head(5)\n",
    "            other = service_costs[5:].sum()\n",
    "            plot_data = pd.concat([top_services, pd.Series({'Others': other})])\n",
    "        else:\n",
    "            plot_data = service_costs\n",
    "        ax1.pie(plot_data.values, labels=plot_data.index, autopct='%1.1f%%', startangle=90)\n",
    "        ax1.set_title('Exadata Cost by Service Type', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        top_skus = sku_analysis['computedAmount'].head(10)\n",
    "        sku_labels = [str(x)[:45] + '...' if len(str(x)) > 45 else str(x) for x in top_skus.index]\n",
    "        ax2.barh(range(len(top_skus)), top_skus.values, color='darkblue')\n",
    "        ax2.set_yticks(range(len(top_skus)))\n",
    "        ax2.set_yticklabels(sku_labels, fontsize=9)\n",
    "        ax2.set_xlabel('Cost (USD)', fontsize=11)\n",
    "        ax2.set_title('Top 10 Exadata SKUs by Cost', fontsize=14, fontweight='bold')\n",
    "        ax2.invert_yaxis()\n",
    "        for i, v in enumerate(top_skus.values):\n",
    "            ax2.text(v, i, f' ${v:,.0f}', va='center', fontsize=9)\n",
    "        \n",
    "        ax3 = fig.add_subplot(gs[1, 0])\n",
    "        shape_costs = shape_analysis['computedAmount']\n",
    "        ax3.barh(range(len(shape_costs)), shape_costs.values, color='teal')\n",
    "        ax3.set_yticks(range(len(shape_costs)))\n",
    "        ax3.set_yticklabels(shape_costs.index, fontsize=10)\n",
    "        ax3.set_xlabel('Cost (USD)', fontsize=11)\n",
    "        ax3.set_title('Exadata Cost by Configuration Size', fontsize=14, fontweight='bold')\n",
    "        ax3.invert_yaxis()\n",
    "        for i, v in enumerate(shape_costs.values):\n",
    "            ax3.text(v, i, f' ${v:,.0f}', va='center', fontsize=10)\n",
    "        \n",
    "        ax4 = fig.add_subplot(gs[1, 1])\n",
    "        region_costs = region_analysis['computedAmount']\n",
    "        ax4.barh(range(len(region_costs)), region_costs.values, color='coral')\n",
    "        ax4.set_yticks(range(len(region_costs)))\n",
    "        ax4.set_yticklabels(region_costs.index, fontsize=10)\n",
    "        ax4.set_xlabel('Cost (USD)', fontsize=11)\n",
    "        ax4.set_title('Exadata Cost by Region', fontsize=14, fontweight='bold')\n",
    "        ax4.invert_yaxis()\n",
    "        for i, v in enumerate(region_costs.values):\n",
    "            ax4.text(v, i, f' ${v:,.0f}', va='center', fontsize=10)\n",
    "        \n",
    "        ax5 = fig.add_subplot(gs[2, :])\n",
    "        comp_costs = compartment_analysis['computedAmount'].head(10)\n",
    "        comp_labels = [str(x[0])[:50] for x in comp_costs.index]\n",
    "        ax5.barh(range(len(comp_costs)), comp_costs.values, color='darkgreen')\n",
    "        ax5.set_yticks(range(len(comp_costs)))\n",
    "        ax5.set_yticklabels(comp_labels, fontsize=10)\n",
    "        ax5.set_xlabel('Cost (USD)', fontsize=11)\n",
    "        ax5.set_title('Top 10 Compartments with Exadata', fontsize=14, fontweight='bold')\n",
    "        ax5.invert_yaxis()\n",
    "        for i, v in enumerate(comp_costs.values):\n",
    "            ax5.text(v, i, f' ${v:,.0f}', va='center', fontsize=10)\n",
    "        \n",
    "        ax6 = fig.add_subplot(gs[3, :])\n",
    "        daily_exadata['date'] = pd.to_datetime(daily_exadata['date'])\n",
    "        ax6.plot(daily_exadata['date'], daily_exadata['daily_cost'], marker='o', linewidth=2, markersize=6, color='darkred')\n",
    "        ax6.axhline(y=daily_exadata['daily_cost'].mean(), color='green', linestyle='--', linewidth=2, label=f\"Mean: ${daily_exadata['daily_cost'].mean():,.2f}\")\n",
    "        ax6.set_xlabel('Date', fontsize=11)\n",
    "        ax6.set_ylabel('Daily Cost (USD)', fontsize=11)\n",
    "        ax6.set_title('Exadata Daily Cost Trend', fontsize=14, fontweight='bold')\n",
    "        ax6.legend(fontsize=10)\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "        plt.setp(ax6.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Visualizations complete\")\n",
    "    \n",
    "    # EXPORTS\n",
    "    print(\"\\n\\nüìÅ EXPORTING EXADATA ANALYSIS DATA:\")\n",
    "    print(\"-\" * 140)\n",
    "    \n",
    "    system_export = system_details[['resourceId', 'resourceName', 'service', 'region_from_call2', \n",
    "                                      'compartment_name_clean', 'sku_clean', 'computedAmount', \n",
    "                                      'computedQuantity', 'hourly_rate']].copy()\n",
    "    system_export.columns = ['Resource_ID', 'Resource_Name', 'Service', 'Region', 'Compartment', \n",
    "                              'SKU', 'Total_Cost', 'Total_Hours', 'Hourly_Rate']\n",
    "    system_export.to_csv('../output/exadata_systems_detail.csv', index=False)\n",
    "    print(f\"‚úÖ Exported: ../output/exadata_systems_detail.csv ({len(system_export)} systems)\")\n",
    "    \n",
    "    sku_export = sku_analysis.reset_index()\n",
    "    sku_export.columns = ['SKU', 'Total_Cost', 'Total_Hours', 'Num_Systems', 'Service', 'Cost_Per_Hour', 'Cost_Per_System']\n",
    "    sku_export.to_csv('../output/exadata_sku_analysis.csv', index=False)\n",
    "    print(f\"‚úÖ Exported: ../output/exadata_sku_analysis.csv ({len(sku_export)} SKUs)\")\n",
    "    \n",
    "    comp_export = compartment_analysis.reset_index()\n",
    "    comp_export.columns = ['Compartment', 'Compartment_Path', 'Total_Cost', 'Num_Systems', 'Total_Hours']\n",
    "    comp_export.to_csv('../output/exadata_compartments.csv', index=False)\n",
    "    print(f\"‚úÖ Exported: ../output/exadata_compartments.csv ({len(comp_export)} compartments)\")\n",
    "    \n",
    "    daily_exadata.columns = ['Date', 'Daily_Cost']\n",
    "    daily_exadata.to_csv('../output/exadata_daily_costs.csv', index=False)\n",
    "    print(f\"‚úÖ Exported: ../output/exadata_daily_costs.csv ({len(daily_exadata)} days)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*140)\n",
    "    print(\"‚úÖ EXADATA ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*140)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No Exadata infrastructure found in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9ab25",
   "metadata": {},
   "source": [
    "## 3. Executive Summary - Key Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eafa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key metrics\n",
    "total_cost = df['computedAmount'].sum()\n",
    "daily_avg_cost = df.groupby('date')['computedAmount'].sum().mean()\n",
    "top_service = df.groupby('service')['computedAmount'].sum().idxmax()\n",
    "top_service_cost = df.groupby('service')['computedAmount'].sum().max()\n",
    "top_region = df.groupby('region_from_call2')['computedAmount'].sum().idxmax()\n",
    "top_compartment = df.groupby('compartment_name_clean')['computedAmount'].sum().idxmax()\n",
    "\n",
    "# Display executive summary\n",
    "print(\"=\"*80)\n",
    "print(\" \"*25 + \"EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüí∞ Total Cost:                  ${total_cost:,.2f}\")\n",
    "print(f\"üìÖ Daily Average Cost:          ${daily_avg_cost:,.2f}\")\n",
    "print(f\"üìä Monthly Projected Cost:      ${daily_avg_cost * 30:,.2f}\")\n",
    "print(f\"\\nüèÜ Top Service:                 {top_service}\")\n",
    "print(f\"   Cost: ${top_service_cost:,.2f} ({top_service_cost/total_cost*100:.1f}%)\")\n",
    "print(f\"\\nüåç Top Region:                  {top_region}\")\n",
    "print(f\"üì¶ Top Compartment:             {top_compartment}\")\n",
    "print(f\"\\nüî¢ Total Resources:             {df['resourceId'].nunique():,}\")\n",
    "print(f\"üî¢ Active Compartments:         {df['compartment_name_clean'].nunique():,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883748dd",
   "metadata": {},
   "source": [
    "## 4. Cost Analysis by Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a93d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 services by cost\n",
    "service_costs = df.groupby('service')['computedAmount'].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "print(\"\\nüìä Top 10 Services by Cost:\")\n",
    "print(\"=\"*80)\n",
    "for idx, (service, cost) in enumerate(service_costs.items(), 1):\n",
    "    pct = (cost/total_cost)*100\n",
    "    print(f\"{idx:2d}. {service:40s} ${cost:>12,.2f} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(14, 6))\n",
    "service_costs.plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('Cost (USD)', fontsize=12)\n",
    "plt.ylabel('Service', fontsize=12)\n",
    "plt.title('Top 10 Services by Cost', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "for i, v in enumerate(service_costs.values):\n",
    "    plt.text(v, i, f' ${v:,.0f}', va='center', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bdbf5b",
   "metadata": {},
   "source": [
    "## 5. Cost Analysis by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b324bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional cost analysis\n",
    "region_costs = df.groupby('region_from_call2')['computedAmount'].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nüåç Cost Distribution by Region:\")\n",
    "print(\"=\"*80)\n",
    "for idx, (region, cost) in enumerate(region_costs.items(), 1):\n",
    "    pct = (cost/total_cost)*100\n",
    "    print(f\"{idx:2d}. {region:30s} ${cost:>12,.2f} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Visualization - Pie chart for top regions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pie chart\n",
    "top_regions = region_costs.head(8)\n",
    "other_cost = region_costs[8:].sum()\n",
    "if other_cost > 0:\n",
    "    plot_data = pd.concat([top_regions, pd.Series({'Others': other_cost})])\n",
    "else:\n",
    "    plot_data = top_regions\n",
    "\n",
    "ax1.pie(plot_data.values, labels=plot_data.index, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Cost Distribution by Region', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "region_costs.head(10).plot(kind='barh', ax=ax2, color='coral')\n",
    "ax2.set_xlabel('Cost (USD)', fontsize=12)\n",
    "ax2.set_ylabel('Region', fontsize=12)\n",
    "ax2.set_title('Top 10 Regions by Cost', fontsize=14, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23877df",
   "metadata": {},
   "source": [
    "## 6. Cost Analysis by Compartment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 compartments by cost\n",
    "compartment_costs = df.groupby('compartment_name_clean')['computedAmount'].sum().sort_values(ascending=False).head(15)\n",
    "\n",
    "print(\"\\nüì¶ Top 15 Compartments by Cost:\")\n",
    "print(\"=\"*80)\n",
    "for idx, (comp, cost) in enumerate(compartment_costs.items(), 1):\n",
    "    pct = (cost/total_cost)*100\n",
    "    print(f\"{idx:2d}. {comp:40s} ${cost:>12,.2f} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(14, 8))\n",
    "compartment_costs.plot(kind='barh', color='teal')\n",
    "plt.xlabel('Cost (USD)', fontsize=12)\n",
    "plt.ylabel('Compartment', fontsize=12)\n",
    "plt.title('Top 15 Compartments by Cost', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "for i, v in enumerate(compartment_costs.values):\n",
    "    plt.text(v, i, f' ${v:,.0f}', va='center', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3566d",
   "metadata": {},
   "source": [
    "## 7. Time Series Analysis - Daily Cost Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use df_trends (output.csv) for more granular time series analysis\n",
    "# Daily cost aggregation from trends data\n",
    "daily_costs = df_trends.groupby('date')['computedAmount'].sum().reset_index()\n",
    "daily_costs['date'] = pd.to_datetime(daily_costs['date'])\n",
    "daily_costs = daily_costs.sort_values('date')\n",
    "\n",
    "# Calculate moving averages\n",
    "daily_costs['MA3'] = daily_costs['computedAmount'].rolling(window=3, min_periods=1).mean()\n",
    "daily_costs['MA7'] = daily_costs['computedAmount'].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "print(\"\\nüìà Daily Cost Statistics (from output.csv):\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Average Daily Cost:    ${daily_costs['computedAmount'].mean():,.2f}\")\n",
    "print(f\"Median Daily Cost:     ${daily_costs['computedAmount'].median():,.2f}\")\n",
    "print(f\"Min Daily Cost:        ${daily_costs['computedAmount'].min():,.2f}\")\n",
    "print(f\"Max Daily Cost:        ${daily_costs['computedAmount'].max():,.2f}\")\n",
    "print(f\"Std Deviation:         ${daily_costs['computedAmount'].std():,.2f}\")\n",
    "print(f\"Total Days:            {len(daily_costs)}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(daily_costs['date'], daily_costs['computedAmount'], marker='o', label='Daily Cost', alpha=0.6, markersize=4)\n",
    "plt.plot(daily_costs['date'], daily_costs['MA3'], label='3-Day Moving Avg', linewidth=2)\n",
    "plt.plot(daily_costs['date'], daily_costs['MA7'], label='7-Day Moving Avg', linewidth=2)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Cost (USD)', fontsize=12)\n",
    "plt.title('Daily Cost Trend with Moving Averages (output.csv)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19eb143",
   "metadata": {},
   "source": [
    "## 8. Service Cost Distribution Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use df_trends (output.csv) for service cost trends over time\n",
    "# Top 5 services cost over time\n",
    "top5_services = df_trends.groupby('service')['computedAmount'].sum().nlargest(5).index\n",
    "service_daily = df_trends[df_trends['service'].isin(top5_services)].groupby(['date', 'service'])['computedAmount'].sum().reset_index()\n",
    "service_daily['date'] = pd.to_datetime(service_daily['date'])\n",
    "\n",
    "print(\"\\nüìä Top 5 Services for Time Series Analysis:\")\n",
    "for idx, service in enumerate(top5_services, 1):\n",
    "    total_cost = df_trends[df_trends['service'] == service]['computedAmount'].sum()\n",
    "    print(f\"{idx}. {service:30s} ${total_cost:>12,.2f}\")\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "for service in top5_services:\n",
    "    service_data = service_daily[service_daily['service'] == service].sort_values('date')\n",
    "    plt.plot(service_data['date'], service_data['computedAmount'], \n",
    "             marker='o', label=service, linewidth=2, alpha=0.7, markersize=4)\n",
    "\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Cost (USD)', fontsize=12)\n",
    "plt.title('Top 5 Services - Daily Cost Trends (output.csv)', fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640078a4",
   "metadata": {},
   "source": [
    "## 9. Compute Instance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd61f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter compute resources\n",
    "compute_df = df[df['service'] == 'Compute'].copy()\n",
    "\n",
    "if len(compute_df) > 0:\n",
    "    # Use skuName for shape analysis (more detailed than shape_from_call2)\n",
    "    # Fill missing skuName with shape_from_call2 as fallback\n",
    "    compute_df['shape_name'] = compute_df['skuName'].fillna(compute_df['shape_from_call2'])\n",
    "    \n",
    "    shape_costs = compute_df.groupby('shape_name')['computedAmount'].sum().sort_values(ascending=False).head(10)\n",
    "    \n",
    "    print(\"\\nüíª Compute Instance Analysis:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total Compute Cost:     ${compute_df['computedAmount'].sum():,.2f}\")\n",
    "    print(f\"Unique Instances:       {compute_df['resourceId'].nunique():,}\")\n",
    "    print(f\"Unique SKUs/Shapes:     {compute_df['shape_name'].nunique():,}\")\n",
    "    \n",
    "    # Shape category analysis (extract shape family from skuName or shape)\n",
    "    compute_df['shape_family'] = compute_df['shape_name'].str.extract(r'^(VM\\.|BM\\.)?([A-Za-z0-9]+)')[1]\n",
    "    shape_family_costs = compute_df.groupby('shape_family')['computedAmount'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nüî¢ Top 10 SKUs/Shapes by Cost:\")\n",
    "    for idx, (shape, cost) in enumerate(shape_costs.items(), 1):\n",
    "        instances = compute_df[compute_df['shape_name'] == shape]['resourceId'].nunique()\n",
    "        avg_cost = cost / instances if instances > 0 else 0\n",
    "        print(f\"{idx:2d}. {shape:50s} ${cost:>12,.2f} ({instances:>3} instances, ${avg_cost:>8,.2f}/instance)\")\n",
    "    \n",
    "    print(\"\\nüìä Cost by Shape Family:\")\n",
    "    for idx, (family, cost) in enumerate(shape_family_costs.head(10).items(), 1):\n",
    "        pct = (cost/compute_df['computedAmount'].sum())*100\n",
    "        instances = compute_df[compute_df['shape_family'] == family]['resourceId'].nunique()\n",
    "        print(f\"{idx:2d}. {family:20s} ${cost:>12,.2f} ({pct:>5.1f}%) - {instances} instances\")\n",
    "    \n",
    "    # Shape distribution by region\n",
    "    print(\"\\nüåç Top SKUs/Shapes by Region:\")\n",
    "    shape_region = compute_df.groupby(['region_from_call2', 'shape_name'])['computedAmount'].sum().reset_index()\n",
    "    shape_region = shape_region.sort_values('computedAmount', ascending=False).head(10)\n",
    "    for idx, row in shape_region.iterrows():\n",
    "        print(f\"   {row['region_from_call2']:20s} | {row['shape_name']:50s} | ${row['computedAmount']:>10,.2f}\")\n",
    "    \n",
    "    # Only create visualizations if there's meaningful cost data\n",
    "    if compute_df['computedAmount'].sum() > 0:\n",
    "        # Visualization - Multiple charts\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        \n",
    "        # 1. Top 10 shapes by cost\n",
    "        shape_costs.plot(kind='barh', ax=ax1, color='darkgreen')\n",
    "        ax1.set_xlabel('Cost (USD)', fontsize=11)\n",
    "        ax1.set_ylabel('SKU/Shape', fontsize=11)\n",
    "        ax1.set_title('Top 10 Compute SKUs/Shapes by Cost', fontsize=12, fontweight='bold')\n",
    "        ax1.invert_yaxis()\n",
    "        for i, v in enumerate(shape_costs.values):\n",
    "            ax1.text(v, i, f' ${v:,.0f}', va='center', fontsize=9)\n",
    "        \n",
    "        # 2. Shape family distribution (pie chart)\n",
    "        top_families = shape_family_costs.head(8)\n",
    "        other_cost = shape_family_costs[8:].sum()\n",
    "        if other_cost > 0:\n",
    "            plot_data = pd.concat([top_families, pd.Series({'Others': other_cost})])\n",
    "        else:\n",
    "            plot_data = top_families\n",
    "        ax2.pie(plot_data.values, labels=plot_data.index, autopct='%1.1f%%', startangle=90)\n",
    "        ax2.set_title('Cost Distribution by Shape Family', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # 3. Instance count by shape\n",
    "        shape_counts = compute_df.groupby('shape_name')['resourceId'].nunique().sort_values(ascending=False).head(10)\n",
    "        shape_counts.plot(kind='barh', ax=ax3, color='steelblue')\n",
    "        ax3.set_xlabel('Number of Instances', fontsize=11)\n",
    "        ax3.set_ylabel('SKU/Shape', fontsize=11)\n",
    "        ax3.set_title('Top 10 SKUs/Shapes by Instance Count', fontsize=12, fontweight='bold')\n",
    "        ax3.invert_yaxis()\n",
    "        for i, v in enumerate(shape_counts.values):\n",
    "            ax3.text(v, i, f' {v}', va='center', fontsize=9)\n",
    "        \n",
    "        # 4. Average cost per instance by shape\n",
    "        shape_avg_cost = compute_df.groupby('shape_name').agg({\n",
    "            'computedAmount': 'sum',\n",
    "            'resourceId': 'nunique'\n",
    "        })\n",
    "        shape_avg_cost['avg_cost'] = shape_avg_cost['computedAmount'] / shape_avg_cost['resourceId']\n",
    "        shape_avg_cost = shape_avg_cost.sort_values('avg_cost', ascending=False).head(10)\n",
    "        \n",
    "        shape_avg_cost['avg_cost'].plot(kind='barh', ax=ax4, color='coral')\n",
    "        ax4.set_xlabel('Average Cost per Instance (USD)', fontsize=11)\n",
    "        ax4.set_ylabel('SKU/Shape', fontsize=11)\n",
    "        ax4.set_title('Top 10 Most Expensive SKUs/Shapes (Avg per Instance)', fontsize=12, fontweight='bold')\n",
    "        ax4.invert_yaxis()\n",
    "        for i, v in enumerate(shape_avg_cost['avg_cost'].values):\n",
    "            ax4.text(v, i, f' ${v:,.0f}', va='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nüìä Skipping visualizations - all costs are zero\")\n",
    "    \n",
    "    # Shape efficiency analysis\n",
    "    print(\"\\n‚ö° SKU/Shape Efficiency Analysis:\")\n",
    "    print(\"=\"*80)\n",
    "    shape_efficiency = compute_df.groupby('shape_name').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    }).reset_index()\n",
    "    shape_efficiency['cost_per_unit'] = shape_efficiency['computedAmount'] / shape_efficiency['computedQuantity'].replace(0, 1)\n",
    "    shape_efficiency = shape_efficiency.sort_values('computedAmount', ascending=False).head(10)\n",
    "    \n",
    "    print(\"\\nTop 10 SKUs/Shapes - Cost vs Usage Metrics:\")\n",
    "    print(f\"{'SKU/Shape':<50} {'Total Cost':>12} {'Total Hours':>12} {'$/Hour':>10} {'Instances':>10}\")\n",
    "    print(\"-\"*100)\n",
    "    for _, row in shape_efficiency.iterrows():\n",
    "        print(f\"{row['shape_name']:<50} ${row['computedAmount']:>11,.2f} {row['computedQuantity']:>12,.1f} ${row['cost_per_unit']:>9,.2f} {row['resourceId']:>10}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No compute resources found in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbacb43a",
   "metadata": {},
   "source": [
    "## 9a. Top Resource Consumers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76adbe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 resource consumers by cost\n",
    "resource_costs = df.groupby(['resourceName', 'service', 'region_from_call2']).agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'computedQuantity': 'sum',\n",
    "    'resourceId': 'first',\n",
    "    'compartment_name_clean': 'first'\n",
    "}).reset_index()\n",
    "resource_costs = resource_costs.sort_values('computedAmount', ascending=False).head(20)\n",
    "\n",
    "print(\"\\nüèÜ Top 20 Resource Consumers by Cost:\")\n",
    "print(\"=\"*120)\n",
    "print(f\"{'Rank':<5} {'Resource Name':<35} {'Service':<20} {'Region':<15} {'Cost':>12} {'Compartment':<25}\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "for idx, row in resource_costs.iterrows():\n",
    "    rank = resource_costs.index.get_loc(idx) + 1\n",
    "    resource_name = str(row['resourceName'])[:33] if pd.notna(row['resourceName']) else 'N/A'\n",
    "    service = str(row['service'])[:18]\n",
    "    region = str(row['region_from_call2'])[:13]\n",
    "    cost = row['computedAmount']\n",
    "    compartment = str(row['compartment_name_clean'])[:23]\n",
    "    \n",
    "    print(f\"{rank:<5} {resource_name:<35} {service:<20} {region:<15} ${cost:>11,.2f} {compartment:<25}\")\n",
    "\n",
    "# Calculate statistics\n",
    "total_top20_cost = resource_costs['computedAmount'].sum()\n",
    "pct_of_total = (total_top20_cost / df['computedAmount'].sum()) * 100\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"Total cost of Top 20 resources:     ${total_top20_cost:,.2f}\")\n",
    "print(f\"Percentage of total cost:           {pct_of_total:.1f}%\")\n",
    "print(f\"Average cost per resource (Top 20): ${resource_costs['computedAmount'].mean():,.2f}\")\n",
    "\n",
    "# Visualization - Top 20 resources\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Bar chart of top 20\n",
    "resource_names_short = resource_costs['resourceName'].apply(lambda x: str(x)[:30] + '...' if pd.notna(x) and len(str(x)) > 30 else str(x))\n",
    "ax1.barh(range(len(resource_costs)), resource_costs['computedAmount'], color='darkblue')\n",
    "ax1.set_yticks(range(len(resource_costs)))\n",
    "ax1.set_yticklabels(resource_names_short, fontsize=9)\n",
    "ax1.set_xlabel('Cost (USD)', fontsize=12)\n",
    "ax1.set_ylabel('Resource Name', fontsize=12)\n",
    "ax1.set_title('Top 20 Resource Consumers by Cost', fontsize=13, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "for i, v in enumerate(resource_costs['computedAmount'].values):\n",
    "    ax1.text(v, i, f' ${v:,.0f}', va='center', fontsize=8)\n",
    "\n",
    "# Service distribution for top 20\n",
    "service_dist = resource_costs.groupby('service')['computedAmount'].sum().sort_values(ascending=False)\n",
    "colors = plt.cm.Set3(range(len(service_dist)))\n",
    "ax2.pie(service_dist.values, labels=service_dist.index, autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "ax2.set_title('Service Distribution of Top 20 Resources', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional breakdown by service\n",
    "print(\"\\nüì¶ Top 20 Resources - Breakdown by Service:\")\n",
    "service_breakdown = resource_costs.groupby('service').agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'resourceName': 'count'\n",
    "}).sort_values('computedAmount', ascending=False)\n",
    "service_breakdown.columns = ['Total_Cost', 'Resource_Count']\n",
    "\n",
    "for service, row in service_breakdown.iterrows():\n",
    "    print(f\"  {service:25s} {row['Resource_Count']:>2} resources | ${row['Total_Cost']:>12,.2f}\")\n",
    "\n",
    "# Regional distribution\n",
    "print(\"\\nüåç Top 20 Resources - Regional Distribution:\")\n",
    "region_breakdown = resource_costs.groupby('region_from_call2').agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'resourceName': 'count'\n",
    "}).sort_values('computedAmount', ascending=False)\n",
    "region_breakdown.columns = ['Total_Cost', 'Resource_Count']\n",
    "\n",
    "for region, row in region_breakdown.iterrows():\n",
    "    print(f\"  {region:25s} {row['Resource_Count']:>2} resources | ${row['Total_Cost']:>12,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c7164",
   "metadata": {},
   "source": [
    "## 10. Storage Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72692bb",
   "metadata": {},
   "source": [
    "## 9b. Database Resources Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec138dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter database services\n",
    "database_services = ['Database', 'Autonomous Database', 'MySQL Database Service', 'NoSQL Database', \n",
    "                     'Oracle Database Service', 'PostgreSQL Database Service']\n",
    "database_df = df[df['service'].str.contains('Database', case=False, na=False)].copy()\n",
    "\n",
    "if len(database_df) > 0:\n",
    "    # Identify unique database services in the data\n",
    "    actual_db_services = database_df['service'].unique()\n",
    "    \n",
    "    print(\"\\nüóÑÔ∏è Database Resources Analysis:\")\n",
    "    print(\"=\"*120)\n",
    "    print(f\"Total Database Cost:        ${database_df['computedAmount'].sum():,.2f}\")\n",
    "    print(f\"Total Database Instances:   {database_df['resourceId'].nunique():,}\")\n",
    "    print(f\"Database Service Types:     {database_df['service'].nunique()}\")\n",
    "    \n",
    "    # Database costs by service type\n",
    "    print(\"\\nüìä Database Costs by Service Type:\")\n",
    "    db_service_costs = database_df.groupby('service')['computedAmount'].sum().sort_values(ascending=False)\n",
    "    for idx, (service, cost) in enumerate(db_service_costs.items(), 1):\n",
    "        pct = (cost/database_df['computedAmount'].sum())*100\n",
    "        instances = database_df[database_df['service'] == service]['resourceId'].nunique()\n",
    "        print(f\"{idx}. {service:40s} ${cost:>12,.2f} ({pct:>5.1f}%) - {instances} instances\")\n",
    "    \n",
    "    # Database resources by SKU\n",
    "    print(\"\\nüíæ Database SKUs/Shapes Analysis:\")\n",
    "    database_df['db_sku'] = database_df['skuName'].fillna(database_df['shape_from_call2'])\n",
    "    db_sku_costs = database_df.groupby('db_sku')['computedAmount'].sum().sort_values(ascending=False).head(10)\n",
    "    \n",
    "    for idx, (sku, cost) in enumerate(db_sku_costs.items(), 1):\n",
    "        instances = database_df[database_df['db_sku'] == sku]['resourceId'].nunique()\n",
    "        avg_cost = cost / instances if instances > 0 else 0\n",
    "        print(f\"{idx:2d}. {str(sku)[:50]:50s} ${cost:>12,.2f} ({instances:>3} instances, ${avg_cost:>8,.2f}/instance)\")\n",
    "    \n",
    "    # Database resources by compartment\n",
    "    print(\"\\nüì¶ Database Costs by Compartment:\")\n",
    "    db_compartment = database_df.groupby(['compartment_name_clean', 'compartmentPath']).agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'resourceId': 'nunique',\n",
    "        'resourceName': lambda x: list(x.unique())[:3]  # Sample of resource names\n",
    "    }).sort_values('computedAmount', ascending=False).head(10)\n",
    "    \n",
    "    print(f\"{'Rank':<5} {'Compartment':<35} {'Cost':>12} {'Instances':>10}\")\n",
    "    print(f\"{'':5} {'Full Path':<100}\")\n",
    "    print(\"-\"*120)\n",
    "    for idx, (comp_tuple, row) in enumerate(db_compartment.iterrows(), 1):\n",
    "        comp_name, comp_path = comp_tuple\n",
    "        pct = (row['computedAmount']/database_df['computedAmount'].sum())*100\n",
    "        print(f\"{idx:<5} {comp_name[:33]:<35} ${row['computedAmount']:>11,.2f} {row['resourceId']:>10} ({pct:>4.1f}%)\")\n",
    "        print(f\"{'':5} {comp_path[:98]}\")\n",
    "        print()\n",
    "    \n",
    "    # Database resources by region\n",
    "    print(\"\\nüåç Database Costs by Region:\")\n",
    "    db_region = database_df.groupby('region_from_call2').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    }).sort_values('computedAmount', ascending=False)\n",
    "    \n",
    "    for idx, (region, row) in enumerate(db_region.iterrows(), 1):\n",
    "        pct = (row['computedAmount']/database_df['computedAmount'].sum())*100\n",
    "        print(f\"{idx:2d}. {region:30s} ${row['computedAmount']:>12,.2f} ({pct:>5.1f}%) - {row['resourceId']} instances\")\n",
    "    \n",
    "    # Top 15 database resources by cost\n",
    "    print(\"\\nüèÜ Top 15 Database Resources by Cost:\")\n",
    "    db_resources = database_df.groupby(['resourceName', 'service', 'region_from_call2', 'compartment_name_clean']).agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum'\n",
    "    }).reset_index().sort_values('computedAmount', ascending=False).head(15)\n",
    "    \n",
    "    print(f\"{'Rank':<5} {'Resource Name':<35} {'Service':<25} {'Region':<15} {'Cost':>12}\")\n",
    "    print(f\"{'':5} {'Compartment':<100}\")\n",
    "    print(\"-\"*120)\n",
    "    for idx, row in db_resources.iterrows():\n",
    "        rank = idx + 1\n",
    "        resource_name = str(row['resourceName'])[:33] if pd.notna(row['resourceName']) else 'N/A'\n",
    "        service = str(row['service'])[:23]\n",
    "        region = str(row['region_from_call2'])[:13]\n",
    "        print(f\"{rank:<5} {resource_name:<35} {service:<25} {region:<15} ${row['computedAmount']:>11,.2f}\")\n",
    "        print(f\"{'':5} {str(row['compartment_name_clean'])[:98]}\")\n",
    "        print()\n",
    "    \n",
    "    # Only create visualizations if there's meaningful cost data\n",
    "    if database_df['computedAmount'].sum() > 0:\n",
    "        # Visualization - Multiple charts\n",
    "        fig = plt.figure(figsize=(18, 12))\n",
    "        gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # 1. Database costs by service type (pie chart)\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        ax1.pie(db_service_costs.values, labels=db_service_costs.index, autopct='%1.1f%%', startangle=90)\n",
    "        ax1.set_title('Database Cost Distribution by Service Type', fontsize=13, fontweight='bold')\n",
    "        \n",
    "        # 2. Top 10 database SKUs by cost (horizontal bar)\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        db_sku_plot = db_sku_costs.head(10)\n",
    "        db_sku_labels = [str(x)[:40] + '...' if len(str(x)) > 40 else str(x) for x in db_sku_plot.index]\n",
    "        ax2.barh(range(len(db_sku_plot)), db_sku_plot.values, color='darkblue')\n",
    "        ax2.set_yticks(range(len(db_sku_plot)))\n",
    "        ax2.set_yticklabels(db_sku_labels, fontsize=9)\n",
    "        ax2.set_xlabel('Cost (USD)', fontsize=11)\n",
    "        ax2.set_ylabel('Database SKU', fontsize=11)\n",
    "        ax2.set_title('Top 10 Database SKUs by Cost', fontsize=13, fontweight='bold')\n",
    "        ax2.invert_yaxis()\n",
    "        for i, v in enumerate(db_sku_plot.values):\n",
    "            ax2.text(v, i, f' ${v:,.0f}', va='center', fontsize=8)\n",
    "        \n",
    "        # 3. Database costs by region (horizontal bar)\n",
    "        ax3 = fig.add_subplot(gs[1, 0])\n",
    "        db_region_plot = db_region['computedAmount'].head(10)\n",
    "        db_region_plot.plot(kind='barh', ax=ax3, color='teal')\n",
    "        ax3.set_xlabel('Cost (USD)', fontsize=11)\n",
    "        ax3.set_ylabel('Region', fontsize=11)\n",
    "        ax3.set_title('Top 10 Regions by Database Cost', fontsize=13, fontweight='bold')\n",
    "        ax3.invert_yaxis()\n",
    "        for i, v in enumerate(db_region_plot.values):\n",
    "            ax3.text(v, i, f' ${v:,.0f}', va='center', fontsize=9)\n",
    "        \n",
    "        # 4. Database costs by compartment (horizontal bar)\n",
    "        ax4 = fig.add_subplot(gs[1, 1])\n",
    "        db_comp_plot = db_compartment['computedAmount'].head(10)\n",
    "        comp_labels = [str(x[0])[:30] for x in db_comp_plot.index]\n",
    "        ax4.barh(range(len(db_comp_plot)), db_comp_plot.values, color='coral')\n",
    "        ax4.set_yticks(range(len(db_comp_plot)))\n",
    "        ax4.set_yticklabels(comp_labels, fontsize=9)\n",
    "        ax4.set_xlabel('Cost (USD)', fontsize=11)\n",
    "        ax4.set_ylabel('Compartment', fontsize=11)\n",
    "        ax4.set_title('Top 10 Compartments by Database Cost', fontsize=13, fontweight='bold')\n",
    "        ax4.invert_yaxis()\n",
    "        for i, v in enumerate(db_comp_plot.values):\n",
    "            ax4.text(v, i, f' ${v:,.0f}', va='center', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nüìä Skipping visualizations - all costs are zero\")\n",
    "    \n",
    "    # Database efficiency analysis\n",
    "    print(\"\\n‚ö° Database Efficiency Analysis:\")\n",
    "    print(\"=\"*120)\n",
    "    db_efficiency = database_df.groupby(['resourceName', 'service']).agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum'\n",
    "    }).reset_index()\n",
    "    db_efficiency['cost_per_unit'] = db_efficiency['computedAmount'] / db_efficiency['computedQuantity'].replace(0, 1)\n",
    "    db_efficiency = db_efficiency.sort_values('computedAmount', ascending=False).head(10)\n",
    "    \n",
    "    print(\"\\nTop 10 Database Resources - Cost vs Usage Metrics:\")\n",
    "    print(f\"{'Resource Name':<40} {'Service':<25} {'Total Cost':>12} {'Total Hours':>12} {'$/Hour':>10}\")\n",
    "    print(\"-\"*120)\n",
    "    for _, row in db_efficiency.iterrows():\n",
    "        resource_name = str(row['resourceName'])[:38] if pd.notna(row['resourceName']) else 'N/A'\n",
    "        service = str(row['service'])[:23]\n",
    "        print(f\"{resource_name:<40} {service:<25} ${row['computedAmount']:>11,.2f} {row['computedQuantity']:>12,.1f} ${row['cost_per_unit']:>9,.2f}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìà Database Summary Statistics:\")\n",
    "    print(f\"Total Database Cost:             ${database_df['computedAmount'].sum():,.2f}\")\n",
    "    print(f\"Average Cost per Database:       ${database_df.groupby('resourceId')['computedAmount'].sum().mean():,.2f}\")\n",
    "    print(f\"Most Expensive Database:         ${database_df.groupby('resourceId')['computedAmount'].sum().max():,.2f}\")\n",
    "    print(f\"Total Database Service Types:    {database_df['service'].nunique()}\")\n",
    "    print(f\"Total Compartments with DBs:     {database_df['compartment_name_clean'].nunique()}\")\n",
    "    print(f\"Total Regions with DBs:          {database_df['region_from_call2'].nunique()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No database resources found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11314aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter storage services\n",
    "storage_services = ['Block Storage', 'Object Storage', 'File Storage', 'Archive Storage']\n",
    "storage_df = df[df['service'].isin(storage_services)].copy()\n",
    "\n",
    "if len(storage_df) > 0:\n",
    "    storage_costs = storage_df.groupby('service')['computedAmount'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nüíæ Storage Analysis:\")\n",
    "    print(\"=\"*120)\n",
    "    print(f\"Total Storage Cost:     ${storage_df['computedAmount'].sum():,.2f}\")\n",
    "    print(f\"Total Storage Usage:    {storage_df['computedQuantity'].sum():,.2f} units\")\n",
    "    \n",
    "    print(\"\\nüìä Storage Costs by Type:\")\n",
    "    for idx, (service, cost) in enumerate(storage_costs.items(), 1):\n",
    "        pct = (cost/storage_df['computedAmount'].sum())*100\n",
    "        usage = storage_df[storage_df['service'] == service]['computedQuantity'].sum()\n",
    "        print(f\"{idx}. {service:30s} ${cost:>12,.2f} ({pct:>5.1f}%) | Usage: {usage:>12,.2f}\")\n",
    "    \n",
    "    # Storage by compartment with full path\n",
    "    print(\"\\nüì¶ Storage Costs by Compartment (with Full Path):\")\n",
    "    storage_compartment = storage_df.groupby(['compartment_name_clean', 'compartmentPath']).agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    }).reset_index().sort_values('computedAmount', ascending=False).head(15)\n",
    "    \n",
    "    print(f\"{'Rank':<5} {'Compartment':<30} {'Cost':>12} {'Usage':>12} {'Resources':>10}\")\n",
    "    print(f\"{'':5} {'Full Path':<100}\")\n",
    "    print(\"-\"*120)\n",
    "    for idx, row in storage_compartment.iterrows():\n",
    "        rank = idx + 1\n",
    "        pct = (row['computedAmount']/storage_df['computedAmount'].sum())*100\n",
    "        print(f\"{rank:<5} {row['compartment_name_clean'][:28]:<30} ${row['computedAmount']:>11,.2f} {row['computedQuantity']:>12,.1f} {row['resourceId']:>10} ({pct:>4.1f}%)\")\n",
    "        print(f\"{'':5} {row['compartmentPath'][:98]}\")\n",
    "        print()\n",
    "    \n",
    "    # Storage by service and compartment (top combinations) with full path\n",
    "    print(\"\\nüîç Top 10 Storage Service-Compartment Combinations:\")\n",
    "    service_comp = storage_df.groupby(['service', 'compartment_name_clean', 'compartmentPath']).agg({\n",
    "        'computedAmount': 'sum'\n",
    "    }).reset_index().sort_values('computedAmount', ascending=False).head(10)\n",
    "    \n",
    "    print(f\"{'Rank':<5} {'Service':<20} {'Compartment':<30} {'Cost':>12}\")\n",
    "    print(f\"{'':5} {'Full Path':<100}\")\n",
    "    print(\"-\"*120)\n",
    "    for idx, row in service_comp.iterrows():\n",
    "        rank = idx + 1\n",
    "        print(f\"{rank:<5} {row['service']:<20} {row['compartment_name_clean'][:28]:<30} ${row['computedAmount']:>11,.2f}\")\n",
    "        print(f\"{'':5} {row['compartmentPath'][:98]}\")\n",
    "        print()\n",
    "    \n",
    "    # Only create visualizations if there's meaningful cost data\n",
    "    if storage_df['computedAmount'].sum() > 0:\n",
    "        # Visualization - Multiple charts\n",
    "        fig = plt.figure(figsize=(18, 12))\n",
    "        gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # 1. Pie chart - Storage type distribution\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        ax1.pie(storage_costs.values, labels=storage_costs.index, autopct='%1.1f%%', startangle=90)\n",
    "        ax1.set_title('Storage Cost Distribution by Type', fontsize=13, fontweight='bold')\n",
    "        \n",
    "        # 2. Bar chart - Storage by region\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        storage_region = storage_df.groupby('region_from_call2')['computedAmount'].sum().sort_values(ascending=False).head(10)\n",
    "        storage_region.plot(kind='barh', ax=ax2, color='orange')\n",
    "        ax2.set_xlabel('Cost (USD)', fontsize=11)\n",
    "        ax2.set_ylabel('Region', fontsize=11)\n",
    "        ax2.set_title('Storage Costs by Region', fontsize=13, fontweight='bold')\n",
    "        ax2.invert_yaxis()\n",
    "        for i, v in enumerate(storage_region.values):\n",
    "            ax2.text(v, i, f' ${v:,.0f}', va='center', fontsize=9)\n",
    "        \n",
    "        # 3. Bar chart - Top 15 compartments\n",
    "        ax3 = fig.add_subplot(gs[1, :])\n",
    "        compartment_costs = storage_compartment.set_index('compartment_name_clean')['computedAmount']\n",
    "        ax3.barh(range(len(compartment_costs)), compartment_costs.values, color='teal')\n",
    "        ax3.set_yticks(range(len(compartment_costs)))\n",
    "        ax3.set_yticklabels([str(x)[:38] for x in compartment_costs.index], fontsize=9)\n",
    "        ax3.set_xlabel('Cost (USD)', fontsize=11)\n",
    "        ax3.set_ylabel('Compartment', fontsize=11)\n",
    "        ax3.set_title('Top 15 Compartments by Storage Cost', fontsize=13, fontweight='bold')\n",
    "        ax3.invert_yaxis()\n",
    "        for i, v in enumerate(compartment_costs.values):\n",
    "            ax3.text(v, i, f' ${v:,.0f}', va='center', fontsize=9)\n",
    "        \n",
    "        # 4. Stacked bar - Storage types by compartment\n",
    "        ax4 = fig.add_subplot(gs[2, :])\n",
    "        top_comps = storage_compartment.head(10)['compartment_name_clean'].values\n",
    "        storage_pivot = storage_df[storage_df['compartment_name_clean'].isin(top_comps)].pivot_table(\n",
    "            index='compartment_name_clean', \n",
    "            columns='service', \n",
    "            values='computedAmount', \n",
    "            aggfunc='sum', \n",
    "            fill_value=0\n",
    "        )\n",
    "        storage_pivot = storage_pivot.reindex(top_comps)  # Maintain order\n",
    "        storage_pivot.plot(kind='barh', stacked=True, ax=ax4, colormap='Set3')\n",
    "        ax4.set_xlabel('Cost (USD)', fontsize=11)\n",
    "        ax4.set_ylabel('Compartment', fontsize=11)\n",
    "        ax4.set_title('Storage Type Distribution by Top 10 Compartments', fontsize=13, fontweight='bold')\n",
    "        ax4.legend(title='Storage Type', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "        ax4.invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nüìä Skipping visualizations - all costs are zero\")\n",
    "    \n",
    "    # Summary statistics by compartment\n",
    "    print(\"\\nüìà Storage Summary by Compartment:\")\n",
    "    print(f\"Total Compartments with Storage:  {storage_df['compartment_name_clean'].nunique()}\")\n",
    "    print(f\"Average Cost per Compartment:     ${storage_df.groupby('compartment_name_clean')['computedAmount'].sum().mean():,.2f}\")\n",
    "    print(f\"Top Compartment Cost:             ${storage_compartment['computedAmount'].max():,.2f}\")\n",
    "    print(f\"Top Compartment Usage:            {storage_compartment['computedQuantity'].max():,.2f} units\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No storage resources found in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504492e6",
   "metadata": {},
   "source": [
    "## 11. Cost Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d44401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies using statistical methods\n",
    "daily_costs_sorted = daily_costs.sort_values('date').reset_index(drop=True)\n",
    "mean_cost = daily_costs_sorted['computedAmount'].mean()\n",
    "std_cost = daily_costs_sorted['computedAmount'].std()\n",
    "\n",
    "# Define anomalies as values beyond 2 standard deviations\n",
    "daily_costs_sorted['is_anomaly'] = np.abs(daily_costs_sorted['computedAmount'] - mean_cost) > (2 * std_cost)\n",
    "anomalies = daily_costs_sorted[daily_costs_sorted['is_anomaly']]\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Cost Anomaly Detection:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Threshold: ¬±2 Standard Deviations from Mean\")\n",
    "print(f\"Mean Daily Cost:      ${mean_cost:,.2f}\")\n",
    "print(f\"Standard Deviation:   ${std_cost:,.2f}\")\n",
    "print(f\"Upper Threshold:      ${mean_cost + 2*std_cost:,.2f}\")\n",
    "print(f\"Lower Threshold:      ${mean_cost - 2*std_cost:,.2f}\")\n",
    "print(f\"\\nAnomalies Detected:   {len(anomalies)}\")\n",
    "\n",
    "if len(anomalies) > 0:\n",
    "    print(\"\\nüìÖ Anomalous Days:\")\n",
    "    for _, row in anomalies.iterrows():\n",
    "        deviation = ((row['computedAmount'] - mean_cost) / mean_cost) * 100\n",
    "        print(f\"  {row['date']}: ${row['computedAmount']:,.2f} ({deviation:+.1f}% from mean)\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(daily_costs_sorted['date'], daily_costs_sorted['computedAmount'], \n",
    "             marker='o', label='Daily Cost', color='blue', alpha=0.6)\n",
    "    plt.axhline(y=mean_cost, color='green', linestyle='--', label='Mean', linewidth=2)\n",
    "    plt.axhline(y=mean_cost + 2*std_cost, color='red', linestyle='--', label='Upper Threshold', linewidth=2)\n",
    "    plt.axhline(y=mean_cost - 2*std_cost, color='red', linestyle='--', label='Lower Threshold', linewidth=2)\n",
    "    \n",
    "    # Highlight anomalies\n",
    "    if len(anomalies) > 0:\n",
    "        plt.scatter(anomalies['date'], anomalies['computedAmount'], \n",
    "                   color='red', s=200, zorder=5, label='Anomalies', marker='X')\n",
    "    \n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Cost (USD)', fontsize=12)\n",
    "    plt.title('Cost Anomaly Detection (¬±2œÉ)', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n‚úÖ No anomalies detected in the period\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd791471",
   "metadata": {},
   "source": [
    "## 12. Cost Optimization Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüí° Cost Optimization Opportunities:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Idle resources (very low usage)\n",
    "resource_usage = df.groupby('resourceId').agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'computedQuantity': 'sum',\n",
    "    'service': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Resources with cost but minimal usage\n",
    "potential_idle = resource_usage[(resource_usage['computedAmount'] > 0) & \n",
    "                                (resource_usage['computedQuantity'] < 1)]\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£ Potentially Idle Resources: {len(potential_idle)}\")\n",
    "if len(potential_idle) > 0:\n",
    "    idle_cost = potential_idle['computedAmount'].sum()\n",
    "    print(f\"   Potential Savings: ${idle_cost:,.2f}\")\n",
    "    print(\"\\n   Top 5 by Cost:\")\n",
    "    for idx, row in potential_idle.nlargest(5, 'computedAmount').iterrows():\n",
    "        print(f\"   - {row['service']:20s} ${row['computedAmount']:>10,.2f}\")\n",
    "\n",
    "# 2. Multi-region resources that could be consolidated\n",
    "multi_region_services = df.groupby('service')['region_from_call2'].nunique()\n",
    "multi_region_services = multi_region_services[multi_region_services > 3].sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£ Services Spanning Multiple Regions (3+):\")\n",
    "for service, count in multi_region_services.head(5).items():\n",
    "    service_cost = df[df['service'] == service]['computedAmount'].sum()\n",
    "    print(f\"   - {service:30s} {count} regions, ${service_cost:,.2f}\")\n",
    "\n",
    "# 3. High-cost compartments that need review\n",
    "high_cost_compartments = df.groupby('compartment_name_clean')['computedAmount'].sum().nlargest(5)\n",
    "print(f\"\\n3Ô∏è‚É£ Top 5 High-Cost Compartments (Review for optimization):\")\n",
    "for comp, cost in high_cost_compartments.items():\n",
    "    pct = (cost/total_cost)*100\n",
    "    print(f\"   - {comp:40s} ${cost:>10,.2f} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nüí° Recommendation: Review Cloud Advisor recommendations for detailed optimization guidance\")\n",
    "print(\"   Run: ./collector.sh <params> --only-recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910c8bd",
   "metadata": {},
   "source": [
    "## 13. Cost Forecast (Simple Linear Projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55475b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear forecast for next 7 days\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Prepare data\n",
    "daily_costs_forecast = daily_costs_sorted.copy()\n",
    "daily_costs_forecast['day_num'] = range(len(daily_costs_forecast))\n",
    "\n",
    "X = daily_costs_forecast[['day_num']].values\n",
    "y = daily_costs_forecast['computedAmount'].values\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Forecast next 7 days\n",
    "future_days = np.array(range(len(daily_costs_forecast), len(daily_costs_forecast) + 7)).reshape(-1, 1)\n",
    "forecast = model.predict(future_days)\n",
    "\n",
    "last_date = daily_costs_forecast['date'].max()\n",
    "forecast_dates = pd.date_range(start=last_date + timedelta(days=1), periods=7)\n",
    "\n",
    "print(\"\\nüìä 7-Day Cost Forecast (Linear Projection):\")\n",
    "print(\"=\"*80)\n",
    "total_forecast = 0\n",
    "for date, cost in zip(forecast_dates, forecast):\n",
    "    print(f\"{date.strftime('%Y-%m-%d')}: ${cost:,.2f}\")\n",
    "    total_forecast += cost\n",
    "print(f\"\\nForecast Total (7 days): ${total_forecast:,.2f}\")\n",
    "print(f\"Forecast Monthly:        ${total_forecast * 30 / 7:,.2f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(daily_costs_forecast['date'], daily_costs_forecast['computedAmount'], \n",
    "         marker='o', label='Historical', color='blue', linewidth=2)\n",
    "plt.plot(forecast_dates, forecast, marker='s', label='Forecast', \n",
    "         color='red', linestyle='--', linewidth=2)\n",
    "plt.axvline(x=last_date, color='gray', linestyle=':', linewidth=2, label='Today')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Cost (USD)', fontsize=12)\n",
    "plt.title('7-Day Cost Forecast', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7644e23",
   "metadata": {},
   "source": [
    "## 14. Export Key Insights to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export top spenders by different dimensions\n",
    "print(\"\\nüìÅ Exporting Key Insights:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Top services\n",
    "top_services_df = df.groupby('service')['computedAmount'].sum().reset_index()\n",
    "top_services_df.columns = ['Service', 'Total_Cost']\n",
    "top_services_df = top_services_df.sort_values('Total_Cost', ascending=False)\n",
    "top_services_df.to_csv('../output/analysis_top_services.csv', index=False)\n",
    "print(f\"‚úÖ Exported: ../output/analysis_top_services.csv ({len(top_services_df)} services)\")\n",
    "\n",
    "# Top compartments\n",
    "top_compartments_df = df.groupby(['compartment_name_clean', 'compartmentPath'])['computedAmount'].sum().reset_index()\n",
    "top_compartments_df.columns = ['Compartment', 'Full_Path', 'Total_Cost']\n",
    "top_compartments_df = top_compartments_df.sort_values('Total_Cost', ascending=False)\n",
    "top_compartments_df.to_csv('../output/analysis_top_compartments.csv', index=False)\n",
    "print(f\"‚úÖ Exported: ../output/analysis_top_compartments.csv ({len(top_compartments_df)} compartments)\")\n",
    "\n",
    "# Daily costs\n",
    "daily_costs_export = daily_costs_sorted[['date', 'computedAmount', 'MA3', 'MA7']]\n",
    "daily_costs_export.columns = ['Date', 'Daily_Cost', 'MA_3Day', 'MA_7Day']\n",
    "daily_costs_export.to_csv('../output/analysis_daily_costs.csv', index=False)\n",
    "print(f\"‚úÖ Exported: ../output/analysis_daily_costs.csv ({len(daily_costs_export)} days)\")\n",
    "\n",
    "# Resource-level analysis\n",
    "resource_summary = df.groupby(['resourceId', 'service', 'region_from_call2']).agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'computedQuantity': 'sum',\n",
    "    'compartment_name_clean': 'first'\n",
    "}).reset_index()\n",
    "resource_summary.columns = ['Resource_ID', 'Service', 'Region', 'Total_Cost', 'Total_Quantity', 'Compartment']\n",
    "resource_summary = resource_summary.sort_values('Total_Cost', ascending=False)\n",
    "resource_summary.to_csv('../output/analysis_resource_summary.csv', index=False)\n",
    "print(f\"‚úÖ Exported: ../output/analysis_resource_summary.csv ({len(resource_summary)} resources)\")\n",
    "\n",
    "print(\"\\n‚úÖ All insights exported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49559d95",
   "metadata": {},
   "source": [
    "## 15. Custom Query Section\n",
    "\n",
    "Use this section to run custom queries on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33730aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Filter by specific service\n",
    "# service_name = 'Compute'\n",
    "# service_data = df[df['service'] == service_name]\n",
    "# print(f\"Total {service_name} cost: ${service_data['computedAmount'].sum():,.2f}\")\n",
    "\n",
    "# Example: Filter by date range\n",
    "# start_date = '2025-11-01'\n",
    "# end_date = '2025-11-10'\n",
    "# date_filtered = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "# print(f\"Cost in date range: ${date_filtered['computedAmount'].sum():,.2f}\")\n",
    "\n",
    "# Add your custom queries here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b060856b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive FinOps intelligence including:\n",
    "- ‚úÖ Executive summary with key metrics\n",
    "- ‚úÖ Cost breakdowns by service, region, and compartment\n",
    "- ‚úÖ Time series analysis and trends\n",
    "- ‚úÖ Compute and storage deep dives\n",
    "- ‚úÖ Anomaly detection\n",
    "- ‚úÖ Cost optimization opportunities\n",
    "- ‚úÖ 7-day cost forecast\n",
    "- ‚úÖ Exportable insights for reporting\n",
    "\n",
    "### Next Steps:\n",
    "1. Review anomalies and investigate root causes\n",
    "2. Analyze high-cost compartments and services\n",
    "3. Check Cloud Advisor recommendations: `./collector.sh <params> --only-recommendations`\n",
    "4. Implement cost optimization strategies\n",
    "5. Set up regular monitoring and reporting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
