{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d3eb82",
   "metadata": {},
   "source": [
    "# Comprehensive Exadata & Database Resources Analysis\n",
    "\n",
    "This notebook provides a deep-dive analysis of all Exadata and database infrastructure resources, including cost tracking, capacity analysis, performance metrics, and optimization opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27735a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Load Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "\n",
    "# Load data from output files\n",
    "try:\n",
    "    file = '../output/1.csv'\n",
    "    df = pd.read_csv(file)\n",
    "    df_trends = pd.read_csv(file)\n",
    "    print(f\"‚úÖ Data loaded successfully\")\n",
    "    print(f\"   Main dataset: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "    print(f\"   Trends dataset: {df_trends.shape[0]:,} rows x {df_trends.shape[1]} columns\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {str(e)}\")\n",
    "    print(\"   Make sure output_merged.csv and output.csv exist in ../output/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd07a5f",
   "metadata": {},
   "source": [
    "## Section 1: Data Preparation & Exadata/Database Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924daa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation and Resource Filtering\n",
    "print(\"=\"*160)\n",
    "print(\" \"*50 + \"DATA PREPARATION & RESOURCE FILTERING\")\n",
    "print(\"=\"*160)\n",
    "\n",
    "# Convert dates\n",
    "df['timeUsageStarted'] = pd.to_datetime(df['timeUsageStarted'])\n",
    "df['timeUsageEnded'] = pd.to_datetime(df['timeUsageEnded'])\n",
    "df['date'] = df['timeUsageStarted'].dt.date\n",
    "df['month'] = df['timeUsageStarted'].dt.to_period('M')\n",
    "\n",
    "# Clean compartment paths\n",
    "df['compartment_name_clean'] = df['compartmentPath'].fillna('Unknown').str.split('/').str[-1]\n",
    "\n",
    "# Ensure string columns for filtering\n",
    "df['service'] = df['service'].fillna('Unknown Service').astype(str)\n",
    "df['region_from_call2'] = df['region_from_call2'].fillna(df['region']).astype(str)\n",
    "\n",
    "# Safe string operations with type conversion\n",
    "for col in ['skuName', 'skuPartNumber', 'resourceName']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna('').astype(str)\n",
    "\n",
    "# ========================================================================================\n",
    "# FILTER 1: EXADATA RESOURCES\n",
    "# ========================================================================================\n",
    "print(\"\\nüìä Filtering Exadata resources...\")\n",
    "filter_exadata = df['service'].str.contains('Exadata', case=False, na=False)\n",
    "\n",
    "if 'skuName' in df.columns:\n",
    "    filter_exadata = filter_exadata | df['skuName'].str.contains('Exadata', case=False, na=False)\n",
    "if 'skuPartNumber' in df.columns:\n",
    "    filter_exadata = filter_exadata | df['skuPartNumber'].str.contains('Exadata', case=False, na=False)\n",
    "if 'resourceName' in df.columns:\n",
    "    filter_exadata = filter_exadata | df['resourceName'].str.contains('Exadata', case=False, na=False)\n",
    "\n",
    "exadata_df = df[filter_exadata].copy()\n",
    "print(f\"‚úÖ Found {len(exadata_df):,} Exadata records\")\n",
    "print(f\"   Total Cost: ${exadata_df['computedAmount'].sum():,.2f}\")\n",
    "print(f\"   Unique Systems: {exadata_df['resourceId'].nunique()}\")\n",
    "\n",
    "# ========================================================================================\n",
    "# FILTER 2: DATABASE RESOURCES\n",
    "# ========================================================================================\n",
    "print(\"\\nüìä Filtering Database resources...\")\n",
    "database_keywords = ['Database', 'Autonomous', 'MySQL', 'PostgreSQL', 'NoSQL', 'Oracle DB']\n",
    "filter_database = df['service'].str.contains('|'.join(database_keywords), case=False, na=False)\n",
    "\n",
    "database_df = df[filter_database].copy()\n",
    "print(f\"‚úÖ Found {len(database_df):,} Database records\")\n",
    "print(f\"   Total Cost: ${database_df['computedAmount'].sum():,.2f}\")\n",
    "print(f\"   Unique Databases: {database_df['resourceId'].nunique()}\")\n",
    "print(f\"   Database Services: {database_df['service'].nunique()}\")\n",
    "\n",
    "# ========================================================================================\n",
    "# FILTER 3: COMBINED EXADATA + DATABASE\n",
    "# ========================================================================================\n",
    "print(\"\\nüìä Filtering Combined Exadata + Database resources...\")\n",
    "filter_combined = filter_exadata | filter_database\n",
    "combined_df = df[filter_combined].copy()\n",
    "\n",
    "print(f\"‚úÖ Found {len(combined_df):,} combined Exadata/Database records\")\n",
    "print(f\"   Total Cost: ${combined_df['computedAmount'].sum():,.2f}\")\n",
    "print(f\"   Percentage of Total Cloud Spend: {(combined_df['computedAmount'].sum() / df['computedAmount'].sum()) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*160)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d752cb8",
   "metadata": {},
   "source": [
    "## Section 1.5: SKU Analysis - Exadata Infrastructure vs CPU Cost Segmentation by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2557a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*160)\n",
    "print(\" \"*45 + \"EXADATA INFRASTRUCTURE vs CPU COST SEGMENTATION\")\n",
    "print(\"=\"*160)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# COST SEGMENTATION: Infrastructure vs CPU\n",
    "# ============================================================================\n",
    "\n",
    "# Keywords to identify CPU-related costs\n",
    "cpu_keywords = ['CPU', 'OCPU', 'OCPUs', 'Compute', 'Core', 'vCPU', 'Processor']\n",
    "infra_keywords = ['Storage', 'Memory', 'Network', 'Backup', 'License', 'Support', 'Infrastructure']\n",
    "\n",
    "# Create cost category flags\n",
    "exadata_df['is_cpu'] = False\n",
    "exadata_df['is_infrastructure'] = False\n",
    "\n",
    "# Safe string operations for SKU identification\n",
    "for col in ['skuName', 'skuPartNumber', 'productName', 'description']:\n",
    "    if col in exadata_df.columns:\n",
    "        exadata_df_clean_col = exadata_df[col].fillna('').astype(str).str.upper()\n",
    "        \n",
    "        # Mark CPU costs\n",
    "        exadata_df['is_cpu'] = exadata_df['is_cpu'] | exadata_df_clean_col.str.contains('|'.join(cpu_keywords), na=False)\n",
    "        \n",
    "        # Mark Infrastructure costs\n",
    "        exadata_df['is_infrastructure'] = exadata_df['is_infrastructure'] | exadata_df_clean_col.str.contains('|'.join(infra_keywords), na=False)\n",
    "\n",
    "# Mark remaining as Other/Misc\n",
    "exadata_df['cost_category'] = 'Other'\n",
    "exadata_df.loc[exadata_df['is_cpu'], 'cost_category'] = 'CPU'\n",
    "exadata_df.loc[exadata_df['is_infrastructure'], 'cost_category'] = 'Infrastructure'\n",
    "\n",
    "# Similarly for Database\n",
    "database_df['is_cpu'] = False\n",
    "database_df['is_infrastructure'] = False\n",
    "\n",
    "for col in ['skuName', 'skuPartNumber', 'productName', 'description']:\n",
    "    if col in database_df.columns:\n",
    "        database_df_clean_col = database_df[col].fillna('').astype(str).str.upper()\n",
    "        database_df['is_cpu'] = database_df['is_cpu'] | database_df_clean_col.str.contains('|'.join(cpu_keywords), na=False)\n",
    "        database_df['is_infrastructure'] = database_df['is_infrastructure'] | database_df_clean_col.str.contains('|'.join(infra_keywords), na=False)\n",
    "\n",
    "database_df['cost_category'] = 'Other'\n",
    "database_df.loc[database_df['is_cpu'], 'cost_category'] = 'CPU'\n",
    "database_df.loc[database_df['is_infrastructure'], 'cost_category'] = 'Infrastructure'\n",
    "\n",
    "# ============================================================================\n",
    "# EXADATA: COST SEGMENTATION SUMMARY\n",
    "# ============================================================================\n",
    "print(\"üìä EXADATA COST SEGMENTATION SUMMARY:\")\n",
    "print(\"-\" * 160)\n",
    "\n",
    "exadata_segmentation = exadata_df.groupby('cost_category').agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'computedQuantity': 'sum',\n",
    "    'resourceId': 'nunique'\n",
    "}).sort_values('computedAmount', ascending=False)\n",
    "\n",
    "total_exadata = exadata_df['computedAmount'].sum()\n",
    "print(f\"{'Category':<25} {'Total Cost':>20} {'% of Exadata':>15} {'Hours':>18} {'Resources':>12}\")\n",
    "print(\"-\" * 160)\n",
    "for category, row in exadata_segmentation.iterrows():\n",
    "    pct = (row['computedAmount'] / total_exadata) * 100\n",
    "    print(f\"{category:<25} ${row['computedAmount']:>19,.2f} {pct:>14.1f}% {row['computedQuantity']:>18,.0f} {row['resourceId']:>12}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# DATABASE: COST SEGMENTATION SUMMARY\n",
    "# ============================================================================\n",
    "print(\"üìä DATABASE COST SEGMENTATION SUMMARY:\")\n",
    "print(\"-\" * 160)\n",
    "\n",
    "database_segmentation = database_df.groupby('cost_category').agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'computedQuantity': 'sum',\n",
    "    'resourceId': 'nunique'\n",
    "}).sort_values('computedAmount', ascending=False)\n",
    "\n",
    "total_database = database_df['computedAmount'].sum()\n",
    "print(f\"{'Category':<25} {'Total Cost':>20} {'% of Database':>15} {'Hours':>18} {'Resources':>12}\")\n",
    "print(\"-\" * 160)\n",
    "for category, row in database_segmentation.iterrows():\n",
    "    pct = (row['computedAmount'] / total_database) * 100\n",
    "    print(f\"{category:<25} ${row['computedAmount']:>19,.2f} {pct:>14.1f}% {row['computedQuantity']:>18,.0f} {row['resourceId']:>12}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# DAILY PROGRESSION: EXADATA\n",
    "# ============================================================================\n",
    "print(\"üìà EXADATA DAILY COST PROGRESSION:\")\n",
    "print(\"-\" * 160)\n",
    "\n",
    "exadata_daily_seg = exadata_df.groupby(['date', 'cost_category']).agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'computedQuantity': 'sum'\n",
    "}).reset_index().sort_values('date')\n",
    "\n",
    "# Pivot for display\n",
    "exadata_daily_pivot = exadata_daily_seg.pivot_table(\n",
    "    index='date',\n",
    "    columns='cost_category',\n",
    "    values='computedAmount',\n",
    "    fill_value=0,\n",
    "    aggfunc='sum'\n",
    ")\n",
    "\n",
    "exadata_daily_pivot['Total'] = exadata_daily_pivot.sum(axis=1)\n",
    "\n",
    "print(f\"{'Date':<12}\", end='')\n",
    "for col in exadata_daily_pivot.columns:\n",
    "    print(f\"{col:>20}\", end='')\n",
    "print()\n",
    "print(\"-\" * 160)\n",
    "\n",
    "for date, row in exadata_daily_pivot.iterrows():\n",
    "    print(f\"{str(date):<12}\", end='')\n",
    "    for value in row:\n",
    "        print(f\"${value:>19,.2f}\", end='')\n",
    "    print()\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# DAILY PROGRESSION: DATABASE\n",
    "# ============================================================================\n",
    "print(\"üìà DATABASE DAILY COST PROGRESSION:\")\n",
    "print(\"-\" * 160)\n",
    "\n",
    "database_daily_seg = database_df.groupby(['date', 'cost_category']).agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'computedQuantity': 'sum'\n",
    "}).reset_index().sort_values('date')\n",
    "\n",
    "# Pivot for display\n",
    "database_daily_pivot = database_daily_seg.pivot_table(\n",
    "    index='date',\n",
    "    columns='cost_category',\n",
    "    values='computedAmount',\n",
    "    fill_value=0,\n",
    "    aggfunc='sum'\n",
    ")\n",
    "\n",
    "database_daily_pivot['Total'] = database_daily_pivot.sum(axis=1)\n",
    "\n",
    "print(f\"{'Date':<12}\", end='')\n",
    "for col in database_daily_pivot.columns:\n",
    "    print(f\"{col:>20}\", end='')\n",
    "print()\n",
    "print(\"-\" * 160)\n",
    "\n",
    "for date, row in database_daily_pivot.iterrows():\n",
    "    print(f\"{str(date):<12}\", end='')\n",
    "    for value in row:\n",
    "        print(f\"${value:>19,.2f}\", end='')\n",
    "    print()\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# EXADATA: DETAILED SKU BREAKDOWN BY CATEGORY\n",
    "# ============================================================================\n",
    "print(\"üìã EXADATA DETAILED SKU BREAKDOWN BY CATEGORY:\")\n",
    "print(\"-\" * 160)\n",
    "\n",
    "exadata_sku_detail = exadata_df.groupby(['cost_category', 'skuName', 'skuPartNumber_from_call2']).agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'computedQuantity': 'sum',\n",
    "    'resourceId': 'nunique'\n",
    "}).reset_index().sort_values(['cost_category', 'computedAmount'], ascending=[True, False])\n",
    "\n",
    "print(f\"{'Category':<20} {'SKU Name':<60} {'SKU Part Number':<20} {'Cost':>18} {'Hours':>15} {'Resources':>10}\")\n",
    "print(\"-\" * 160)\n",
    "\n",
    "for _, row in exadata_sku_detail.iterrows():\n",
    "    category = str(row['cost_category'])[:19]\n",
    "    sku_name = str(row['skuName'])[:58]\n",
    "    sku_part = str(row['skuPartNumber_from_call2'])[:18] if pd.notna(row['skuPartNumber_from_call2']) else 'N/A'\n",
    "    cost = f\"${row['computedAmount']:>17,.2f}\"\n",
    "    hours = f\"{row['computedQuantity']:>15,.0f}\"\n",
    "    resources = f\"{row['resourceId']:>10}\"\n",
    "    print(f\"{category:<20} {sku_name:<60} {sku_part:<20} {cost} {hours} {resources}\")\n",
    "\n",
    "print()\n",
    "print(\"\\nüìã EXADATA SKU BREAKDOWN - TABLE VIEW:\")\n",
    "display(exadata_sku_detail[['cost_category', 'skuName', 'skuPartNumber_from_call2', 'computedAmount', 'computedQuantity', 'resourceId']].rename(columns={\n",
    "    'cost_category': 'Category',\n",
    "    'skuName': 'SKU Name',\n",
    "    'skuPartNumber_from_call2': 'SKU Part Number',\n",
    "    'computedAmount': 'Total Cost',\n",
    "    'computedQuantity': 'Hours',\n",
    "    'resourceId': 'Resources'\n",
    "}))\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# DATABASE: DETAILED SKU BREAKDOWN BY CATEGORY\n",
    "# ============================================================================\n",
    "print(\"üìã DATABASE DETAILED SKU BREAKDOWN BY CATEGORY:\")\n",
    "print(\"-\" * 160)\n",
    "\n",
    "database_sku_detail = database_df.groupby(['cost_category', 'skuName', 'skuPartNumber_from_call2']).agg({\n",
    "    'computedAmount': 'sum',\n",
    "    'computedQuantity': 'sum',\n",
    "    'resourceId': 'nunique'\n",
    "}).reset_index().sort_values(['cost_category', 'computedAmount'], ascending=[True, False])\n",
    "\n",
    "print(f\"{'Category':<20} {'SKU Name':<60} {'SKU Part Number':<20} {'Cost':>18} {'Hours':>15} {'Resources':>10}\")\n",
    "print(\"-\" * 160)\n",
    "\n",
    "for _, row in database_sku_detail.iterrows():\n",
    "    category = str(row['cost_category'])[:19]\n",
    "    sku_name = str(row['skuName'])[:58]\n",
    "    sku_part = str(row['skuPartNumber_from_call2'])[:18] if pd.notna(row['skuPartNumber_from_call2']) else 'N/A'\n",
    "    cost = f\"${row['computedAmount']:>17,.2f}\"\n",
    "    hours = f\"{row['computedQuantity']:>15,.0f}\"\n",
    "    resources = f\"{row['resourceId']:>10}\"\n",
    "    print(f\"{category:<20} {sku_name:<60} {sku_part:<20} {cost} {hours} {resources}\")\n",
    "\n",
    "print()\n",
    "print(\"\\nüìã DATABASE SKU BREAKDOWN - TABLE VIEW:\")\n",
    "display(database_sku_detail[['cost_category', 'skuName', 'skuPartNumber_from_call2', 'computedAmount', 'computedQuantity', 'resourceId']].rename(columns={\n",
    "    'cost_category': 'Category',\n",
    "    'skuName': 'SKU Name',\n",
    "    'skuPartNumber_from_call2': 'SKU Part Number',\n",
    "    'computedAmount': 'Total Cost',\n",
    "    'computedQuantity': 'Hours',\n",
    "    'resourceId': 'Resources'\n",
    "}))\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATIONS: COST SEGMENTATION TRENDS\n",
    "# ============================================================================\n",
    "print(\"üìä GENERATING COST SEGMENTATION VISUALIZATIONS...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "# 1. Exadata Segmentation Pie Chart\n",
    "ax = axes[0, 0]\n",
    "exadata_by_category = exadata_df.groupby('cost_category')['computedAmount'].sum().sort_values(ascending=False)\n",
    "colors_ex = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "wedges, texts, autotexts = ax.pie(exadata_by_category, labels=exadata_by_category.index, autopct='%1.1f%%', colors=colors_ex, startangle=90)\n",
    "# Add cost labels\n",
    "for i, (wedge, value) in enumerate(zip(wedges, exadata_by_category.values)):\n",
    "    angle = (wedge.theta2 + wedge.theta1) / 2\n",
    "    x = np.cos(np.radians(angle))\n",
    "    y = np.sin(np.radians(angle))\n",
    "    ax.text(0.7*x, 0.7*y, f'${value:,.0f}', ha='center', va='center', fontsize=9, fontweight='bold', color='white')\n",
    "ax.set_title('Exadata Cost Distribution by Category', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. Database Segmentation Pie Chart\n",
    "ax = axes[0, 1]\n",
    "database_by_category = database_df.groupby('cost_category')['computedAmount'].sum().sort_values(ascending=False)\n",
    "colors_db = ['#d62728', '#9467bd', '#8c564b']\n",
    "wedges, texts, autotexts = ax.pie(database_by_category, labels=database_by_category.index, autopct='%1.1f%%', colors=colors_db, startangle=90)\n",
    "# Add cost labels\n",
    "for i, (wedge, value) in enumerate(zip(wedges, database_by_category.values)):\n",
    "    angle = (wedge.theta2 + wedge.theta1) / 2\n",
    "    x = np.cos(np.radians(angle))\n",
    "    y = np.sin(np.radians(angle))\n",
    "    ax.text(0.7*x, 0.7*y, f'${value:,.0f}', ha='center', va='center', fontsize=9, fontweight='bold', color='white')\n",
    "ax.set_title('Database Cost Distribution by Category', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. Exadata Daily Trend by Category\n",
    "ax = axes[1, 0]\n",
    "color_map = {'CPU': '#ff7f0e', 'Infrastructure': '#1f77b4', 'Other': '#2ca02c', 'Total': 'black'}\n",
    "\n",
    "for category in exadata_daily_pivot.columns:\n",
    "    if category != 'Total':\n",
    "        ax.plot(exadata_daily_pivot.index, exadata_daily_pivot[category], marker='o', label=category, linewidth=2, color=color_map.get(category, None))\n",
    "\n",
    "ax.plot(exadata_daily_pivot.index, exadata_daily_pivot['Total'], marker='s', label='Total', linewidth=3, linestyle='--', color='black')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Cost (USD)')\n",
    "ax.set_title('Exadata Daily Cost Progression by Category', fontsize=12, fontweight='bold')\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 4. Database Daily Trend by Category\n",
    "ax = axes[1, 1]\n",
    "for category in database_daily_pivot.columns:\n",
    "    if category != 'Total':\n",
    "        ax.plot(database_daily_pivot.index, database_daily_pivot[category], marker='o', label=category, linewidth=2, color=color_map.get(category, None))\n",
    "\n",
    "ax.plot(database_daily_pivot.index, database_daily_pivot['Total'], marker='s', label='Total', linewidth=3, linestyle='--', color='black')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Cost (USD)')\n",
    "ax.set_title('Database Daily Cost Progression by Category', fontsize=12, fontweight='bold')\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Cost Segmentation Analysis Complete\")\n",
    "print(\"=\"*160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25155839",
   "metadata": {},
   "source": [
    "## Section 2: Exadata Infrastructure - Comprehensive Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(exadata_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*160)\n",
    "    print(\" \"*50 + \"üè¢ EXADATA INFRASTRUCTURE ANALYSIS\")\n",
    "    print(\"=\"*160)\n",
    "    \n",
    "    # Prepare SKU information\n",
    "    exadata_df_work = exadata_df.copy()\n",
    "    exadata_df_work['sku_clean'] = exadata_df_work['skuName'].fillna(exadata_df_work.get('skuPartNumber', '')).fillna('Unknown SKU')\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # EXECUTIVE SUMMARY\n",
    "    # ========================================================================================\n",
    "    total_exadata_cost = exadata_df_work['computedAmount'].sum()\n",
    "    total_exadata_hours = exadata_df_work['computedQuantity'].sum()\n",
    "    unique_systems = exadata_df_work['resourceId'].nunique()\n",
    "    avg_hourly_rate = total_exadata_cost / total_exadata_hours if total_exadata_hours > 0 else 0\n",
    "    \n",
    "    print(\"\\nüìä EXECUTIVE SUMMARY:\")\n",
    "    print(\"-\" * 160)\n",
    "    print(f\"{'Total Exadata Spend:':<40} ${total_exadata_cost:>22,.2f}\")\n",
    "    print(f\"{'Percentage of Total Cloud Spend:':<40} {(total_exadata_cost/df['computedAmount'].sum())*100:>21.2f}%\")\n",
    "    print(f\"{'Total Compute Hours:':<40} {total_exadata_hours:>22,.2f}\")\n",
    "    print(f\"{'Average Hourly Rate:':<40} ${avg_hourly_rate:>22,.2f}\")\n",
    "    print(f\"{'Unique Exadata Systems:':<40} {unique_systems:>22,}\")\n",
    "    print(f\"{'Unique Compartments:':<40} {exadata_df_work['compartment_name_clean'].nunique():>22,}\")\n",
    "    print(f\"{'Regions with Exadata:':<40} {exadata_df_work['region_from_call2'].nunique():>22,}\")\n",
    "    print(f\"{'Date Range:':<40} {str(exadata_df_work['timeUsageStarted'].min())[:10]} to {str(exadata_df_work['timeUsageEnded'].max())[:10]:>6}\")\n",
    "    print(f\"{'Projected Monthly Cost (current rate):':<40} ${avg_hourly_rate * 24 * 30:>22,.2f}\")\n",
    "    print(f\"{'Projected Annual Cost (current rate):':<40} ${avg_hourly_rate * 24 * 365:>22,.2f}\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # SERVICE TYPE BREAKDOWN\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüíº EXADATA SERVICE TYPE BREAKDOWN:\")\n",
    "    print(\"-\" * 160)\n",
    "    service_breakdown = exadata_df_work.groupby('service').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    }).sort_values('computedAmount', ascending=False)\n",
    "    \n",
    "    print(f\"{'Service Type':<50} {'Total Cost':>18} {'% of Total':>12} {'Hours':>18} {'Systems':>10}\")\n",
    "    print(\"-\" * 160)\n",
    "    for service, row in service_breakdown.iterrows():\n",
    "        pct = (row['computedAmount'] / total_exadata_cost) * 100\n",
    "        print(f\"{service:<50} ${row['computedAmount']:>17,.2f} {pct:>11.1f}% {row['computedQuantity']:>18,.1f} {row['resourceId']:>10}\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # SKU/CONFIGURATION ANALYSIS\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüîß SKU/CONFIGURATION ANALYSIS (Top 15):\")\n",
    "    print(\"-\" * 160)\n",
    "    sku_analysis = exadata_df_work.groupby('sku_clean').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    }).sort_values('computedAmount', ascending=False).head(15)\n",
    "    \n",
    "    sku_analysis['cost_per_hour'] = sku_analysis['computedAmount'] / sku_analysis['computedQuantity'].replace(0, 1)\n",
    "    sku_analysis['cost_per_system'] = sku_analysis['computedAmount'] / sku_analysis['resourceId']\n",
    "    \n",
    "    print(f\"{'SKU/Configuration':<80} {'Total Cost':>18} {'$/Hour':>12} {'$/System':>15}\")\n",
    "    print(\"-\" * 160)\n",
    "    for sku, row in sku_analysis.iterrows():\n",
    "        sku_display = str(sku)[:78]\n",
    "        print(f\"{sku_display:<80} ${row['computedAmount']:>17,.2f} ${row['cost_per_hour']:>11,.2f} ${row['cost_per_system']:>14,.2f}\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # REGIONAL DISTRIBUTION\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüåç REGIONAL DISTRIBUTION:\")\n",
    "    print(\"-\" * 160)\n",
    "    region_breakdown = exadata_df_work.groupby('region_from_call2').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    }).sort_values('computedAmount', ascending=False)\n",
    "    \n",
    "    print(f\"{'Region':<40} {'Total Cost':>18} {'% of Total':>12} {'Systems':>12}\")\n",
    "    print(\"-\" * 160)\n",
    "    for region, row in region_breakdown.iterrows():\n",
    "        pct = (row['computedAmount'] / total_exadata_cost) * 100\n",
    "        print(f\"{region:<40} ${row['computedAmount']:>17,.2f} {pct:>11.1f}% {row['resourceId']:>12}\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # COMPARTMENT DISTRIBUTION (Top 20)\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüì¶ COMPARTMENT DISTRIBUTION (Top 20):\")\n",
    "    print(\"-\" * 160)\n",
    "    compartment_breakdown = exadata_df_work.groupby(['compartment_name_clean', 'compartmentPath']).agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    }).sort_values('computedAmount', ascending=False).head(20)\n",
    "    \n",
    "    print(f\"{'Rank':<6} {'Compartment':<45} {'Total Cost':>18} {'% Total':>10} {'Systems':>10}\")\n",
    "    print(f\"{'':6} {'Full Path':<150}\")\n",
    "    print(\"-\" * 160)\n",
    "    for idx, ((comp_name, comp_path), row) in enumerate(compartment_breakdown.iterrows(), 1):\n",
    "        pct = (row['computedAmount'] / total_exadata_cost) * 100\n",
    "        print(f\"{idx:<6} {comp_name[:43]:<45} ${row['computedAmount']:>17,.2f} {pct:>9.1f}% {row['resourceId']:>10}\")\n",
    "        print(f\"{'':6} {comp_path[:148]}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # TOP 25 INDIVIDUAL EXADATA SYSTEMS\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüñ•Ô∏è  TOP 25 INDIVIDUAL EXADATA SYSTEMS BY COST:\")\n",
    "    print(\"-\" * 160)\n",
    "    system_details = exadata_df_work.groupby(['resourceId', 'resourceName', 'service', 'region_from_call2']).agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum'\n",
    "    }).reset_index().sort_values('computedAmount', ascending=False).head(25)\n",
    "    \n",
    "    system_details['hourly_rate'] = system_details['computedAmount'] / system_details['computedQuantity'].replace(0, 1)\n",
    "    \n",
    "    print(f\"{'Rank':<6} {'Resource Name':<45} {'Service':<30} {'Total Cost':>18}\")\n",
    "    print(f\"{'':6} {'Region':<30} {'Monthly Projection':>18} {'Daily Cost':>18}\")\n",
    "    print(\"-\" * 160)\n",
    "    for idx, row in system_details.iterrows():\n",
    "        rank = system_details.index.get_loc(idx) + 1\n",
    "        print(f\"{rank:<6} {str(row['resourceName'])[:43]:<45} {str(row['service'])[:28]:<30} ${row['computedAmount']:>17,.2f}\")\n",
    "        print(f\"{'':6} {str(row['region_from_call2']):<30} ${row['computedAmount']/30*30:>17,.2f} ${row['computedAmount']/30:>17,.2f}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # DAILY COST TREND\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüìà DAILY COST TREND:\")\n",
    "    print(\"-\" * 160)\n",
    "    daily_exadata = exadata_df_work.groupby('date').agg({\n",
    "        'computedAmount': 'sum'\n",
    "    }).reset_index()\n",
    "    daily_exadata['date'] = pd.to_datetime(daily_exadata['date'])\n",
    "    \n",
    "    print(f\"Average Daily Cost: ${daily_exadata['computedAmount'].mean():,.2f}\")\n",
    "    print(f\"Median Daily Cost:  ${daily_exadata['computedAmount'].median():,.2f}\")\n",
    "    print(f\"Min Daily Cost:     ${daily_exadata['computedAmount'].min():,.2f}\")\n",
    "    print(f\"Max Daily Cost:     ${daily_exadata['computedAmount'].max():,.2f}\")\n",
    "    print(f\"Std Deviation:      ${daily_exadata['computedAmount'].std():,.2f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    \n",
    "    # 1. Service distribution\n",
    "    ax = axes[0, 0]\n",
    "    service_breakdown['computedAmount'].head(8).plot(kind='barh', ax=ax, color='steelblue')\n",
    "    ax.set_xlabel('Cost (USD)')\n",
    "    ax.set_title('Exadata Cost by Service Type', fontsize=12, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # 2. Regional distribution\n",
    "    ax = axes[0, 1]\n",
    "    region_breakdown['computedAmount'].plot(kind='barh', ax=ax, color='coral')\n",
    "    ax.set_xlabel('Cost (USD)')\n",
    "    ax.set_title('Exadata Cost by Region', fontsize=12, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # 3. Top 10 systems\n",
    "    ax = axes[1, 0]\n",
    "    top_systems = system_details.head(10).sort_values('computedAmount')\n",
    "    top_systems.set_index(top_systems['resourceName'].str[:30])['computedAmount'].plot(kind='barh', ax=ax, color='teal')\n",
    "    ax.set_xlabel('Cost (USD)')\n",
    "    ax.set_title('Top 10 Exadata Systems', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 4. Daily trend\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(daily_exadata['date'], daily_exadata['computedAmount'], marker='o', linewidth=2, color='darkblue')\n",
    "    ax.axhline(y=daily_exadata['computedAmount'].mean(), color='red', linestyle='--', label='Mean')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Cost (USD)')\n",
    "    ax.set_title('Daily Exadata Cost Trend', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Exadata Analysis Complete\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No Exadata infrastructure found in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc87c58",
   "metadata": {},
   "source": [
    "## Section 3: Database Infrastructure - Comprehensive Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(database_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*160)\n",
    "    print(\" \"*50 + \"üóÑÔ∏è  DATABASE INFRASTRUCTURE ANALYSIS\")\n",
    "    print(\"=\"*160)\n",
    "    \n",
    "    # Prepare SKU information\n",
    "    database_df_work = database_df.copy()\n",
    "    database_df_work['sku_clean'] = database_df_work['skuName'].fillna(database_df_work.get('skuPartNumber', '')).fillna('Unknown SKU')\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # EXECUTIVE SUMMARY\n",
    "    # ========================================================================================\n",
    "    total_db_cost = database_df_work['computedAmount'].sum()\n",
    "    total_db_hours = database_df_work['computedQuantity'].sum()\n",
    "    unique_databases = database_df_work['resourceId'].nunique()\n",
    "    avg_db_hourly_rate = total_db_cost / total_db_hours if total_db_hours > 0 else 0\n",
    "    \n",
    "    print(\"\\nüìä EXECUTIVE SUMMARY:\")\n",
    "    print(\"-\" * 160)\n",
    "    print(f\"{'Total Database Spend:':<40} ${total_db_cost:>22,.2f}\")\n",
    "    print(f\"{'Percentage of Total Cloud Spend:':<40} {(total_db_cost/df['computedAmount'].sum())*100:>21.2f}%\")\n",
    "    print(f\"{'Total Compute Hours:':<40} {total_db_hours:>22,.2f}\")\n",
    "    print(f\"{'Average Hourly Rate:':<40} ${avg_db_hourly_rate:>22,.2f}\")\n",
    "    print(f\"{'Unique Database Instances:':<40} {unique_databases:>22,}\")\n",
    "    print(f\"{'Database Service Types:':<40} {database_df_work['service'].nunique():>22,}\")\n",
    "    print(f\"{'Unique Compartments:':<40} {database_df_work['compartment_name_clean'].nunique():>22,}\")\n",
    "    print(f\"{'Regions with Databases:':<40} {database_df_work['region_from_call2'].nunique():>22,}\")\n",
    "    print(f\"{'Date Range:':<40} {str(database_df_work['timeUsageStarted'].min())[:10]} to {str(database_df_work['timeUsageEnded'].max())[:10]:>6}\")\n",
    "    print(f\"{'Projected Monthly Cost (current rate):':<40} ${avg_db_hourly_rate * 24 * 30:>22,.2f}\")\n",
    "    print(f\"{'Projected Annual Cost (current rate):':<40} ${avg_db_hourly_rate * 24 * 365:>22,.2f}\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # DATABASE SERVICE TYPE BREAKDOWN\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüìã DATABASE SERVICE TYPE BREAKDOWN:\")\n",
    "    print(\"-\" * 160)\n",
    "    db_service_breakdown = database_df_work.groupby('service').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    }).sort_values('computedAmount', ascending=False)\n",
    "    \n",
    "    print(f\"{'Database Service':<50} {'Total Cost':>18} {'% of Total':>12} {'Hours':>18} {'Instances':>10}\")\n",
    "    print(\"-\" * 160)\n",
    "    for service, row in db_service_breakdown.iterrows():\n",
    "        pct = (row['computedAmount'] / total_db_cost) * 100\n",
    "        print(f\"{service:<50} ${row['computedAmount']:>17,.2f} {pct:>11.1f}% {row['computedQuantity']:>18,.1f} {row['resourceId']:>10}\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # DATABASE SKU/SHAPE ANALYSIS\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüîß DATABASE SKU/SHAPE ANALYSIS (Top 20):\")\n",
    "    print(\"-\" * 160)\n",
    "    db_sku_analysis = database_df_work.groupby('sku_clean').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    }).sort_values('computedAmount', ascending=False).head(20)\n",
    "    \n",
    "    db_sku_analysis['cost_per_hour'] = db_sku_analysis['computedAmount'] / db_sku_analysis['computedQuantity'].replace(0, 1)\n",
    "    db_sku_analysis['cost_per_instance'] = db_sku_analysis['computedAmount'] / db_sku_analysis['resourceId']\n",
    "    \n",
    "    print(f\"{'Database SKU/Shape':<80} {'Total Cost':>18} {'$/Hour':>12} {'$/Instance':>15}\")\n",
    "    print(\"-\" * 160)\n",
    "    for sku, row in db_sku_analysis.iterrows():\n",
    "        sku_display = str(sku)[:78]\n",
    "        print(f\"{sku_display:<80} ${row['computedAmount']:>17,.2f} ${row['cost_per_hour']:>11,.2f} ${row['cost_per_instance']:>14,.2f}\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # DATABASE REGIONAL DISTRIBUTION\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüåç DATABASE REGIONAL DISTRIBUTION:\")\n",
    "    print(\"-\" * 160)\n",
    "    db_region_breakdown = database_df_work.groupby('region_from_call2').agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    }).sort_values('computedAmount', ascending=False)\n",
    "    \n",
    "    print(f\"{'Region':<40} {'Total Cost':>18} {'% of Total':>12} {'Instances':>12}\")\n",
    "    print(\"-\" * 160)\n",
    "    for region, row in db_region_breakdown.iterrows():\n",
    "        pct = (row['computedAmount'] / total_db_cost) * 100\n",
    "        print(f\"{region:<40} ${row['computedAmount']:>17,.2f} {pct:>11.1f}% {row['resourceId']:>12}\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # DATABASE COMPARTMENT DISTRIBUTION (Top 20)\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüì¶ DATABASE COMPARTMENT DISTRIBUTION (Top 20):\")\n",
    "    print(\"-\" * 160)\n",
    "    db_compartment_breakdown = database_df_work.groupby(['compartment_name_clean', 'compartmentPath']).agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'resourceId': 'nunique'\n",
    "    }).sort_values('computedAmount', ascending=False).head(20)\n",
    "    \n",
    "    print(f\"{'Rank':<6} {'Compartment':<45} {'Total Cost':>18} {'% Total':>10} {'Instances':>12}\")\n",
    "    print(f\"{'':6} {'Full Path':<150}\")\n",
    "    print(\"-\" * 160)\n",
    "    for idx, ((comp_name, comp_path), row) in enumerate(db_compartment_breakdown.iterrows(), 1):\n",
    "        pct = (row['computedAmount'] / total_db_cost) * 100\n",
    "        print(f\"{idx:<6} {comp_name[:43]:<45} ${row['computedAmount']:>17,.2f} {pct:>9.1f}% {row['resourceId']:>12}\")\n",
    "        print(f\"{'':6} {comp_path[:148]}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # TOP 30 INDIVIDUAL DATABASE INSTANCES\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüìä TOP 30 INDIVIDUAL DATABASE INSTANCES BY COST:\")\n",
    "    print(\"-\" * 160)\n",
    "    db_instances = database_df_work.groupby(['resourceId', 'resourceName', 'service', 'region_from_call2', 'compartment_name_clean']).agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum'\n",
    "    }).reset_index().sort_values('computedAmount', ascending=False).head(30)\n",
    "    \n",
    "    db_instances['hourly_rate'] = db_instances['computedAmount'] / db_instances['computedQuantity'].replace(0, 1)\n",
    "    \n",
    "    print(f\"{'Rank':<6} {'Database Instance':<45} {'Service':<25} {'Total Cost':>18}\")\n",
    "    print(f\"{'':6} {'Region':<25} {'Compartment':<40} {'Daily Cost':>18}\")\n",
    "    print(\"-\" * 160)\n",
    "    for idx, row in db_instances.iterrows():\n",
    "        rank = db_instances.index.get_loc(idx) + 1\n",
    "        print(f\"{rank:<6} {str(row['resourceName'])[:43]:<45} {str(row['service'])[:23]:<25} ${row['computedAmount']:>17,.2f}\")\n",
    "        print(f\"{'':6} {str(row['region_from_call2']):<25} {str(row['compartment_name_clean'])[:38]:<40} ${row['computedAmount']/30:>17,.2f}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # DAILY COST TREND\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüìà DAILY DATABASE COST TREND:\")\n",
    "    print(\"-\" * 160)\n",
    "    daily_db = database_df_work.groupby('date').agg({\n",
    "        'computedAmount': 'sum'\n",
    "    }).reset_index()\n",
    "    daily_db['date'] = pd.to_datetime(daily_db['date'])\n",
    "    \n",
    "    print(f\"Average Daily Cost: ${daily_db['computedAmount'].mean():,.2f}\")\n",
    "    print(f\"Median Daily Cost:  ${daily_db['computedAmount'].median():,.2f}\")\n",
    "    print(f\"Min Daily Cost:     ${daily_db['computedAmount'].min():,.2f}\")\n",
    "    print(f\"Max Daily Cost:     ${daily_db['computedAmount'].max():,.2f}\")\n",
    "    print(f\"Std Deviation:      ${daily_db['computedAmount'].std():,.2f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    \n",
    "    # 1. Service distribution\n",
    "    ax = axes[0, 0]\n",
    "    db_service_breakdown['computedAmount'].head(10).plot(kind='barh', ax=ax, color='darkgreen')\n",
    "    ax.set_xlabel('Cost (USD)')\n",
    "    ax.set_title('Database Cost by Service Type', fontsize=12, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # 2. Regional distribution\n",
    "    ax = axes[0, 1]\n",
    "    db_region_breakdown['computedAmount'].plot(kind='barh', ax=ax, color='purple')\n",
    "    ax.set_xlabel('Cost (USD)')\n",
    "    ax.set_title('Database Cost by Region', fontsize=12, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # 3. Top 10 instances\n",
    "    ax = axes[1, 0]\n",
    "    top_db_instances = db_instances.head(10).sort_values('computedAmount')\n",
    "    top_db_instances.set_index(top_db_instances['resourceName'].str[:30])['computedAmount'].plot(kind='barh', ax=ax, color='orange')\n",
    "    ax.set_xlabel('Cost (USD)')\n",
    "    ax.set_title('Top 10 Database Instances', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 4. Daily trend\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(daily_db['date'], daily_db['computedAmount'], marker='s', linewidth=2, color='darkred')\n",
    "    ax.axhline(y=daily_db['computedAmount'].mean(), color='blue', linestyle='--', label='Mean')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Cost (USD)')\n",
    "    ax.set_title('Daily Database Cost Trend', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Database Analysis Complete\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No Database infrastructure found in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949424fd",
   "metadata": {},
   "source": [
    "## Section 4: Combined Exadata + Database Analysis & Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(combined_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*160)\n",
    "    print(\" \"*45 + \"üìä COMBINED EXADATA & DATABASE ANALYSIS\")\n",
    "    print(\"=\"*160)\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # COMBINED EXECUTIVE SUMMARY\n",
    "    # ========================================================================================\n",
    "    combined_cost = combined_df['computedAmount'].sum()\n",
    "    combined_pct_total = (combined_cost / df['computedAmount'].sum()) * 100\n",
    "    \n",
    "    exadata_cost = exadata_df['computedAmount'].sum() if len(exadata_df) > 0 else 0\n",
    "    exadata_pct = (exadata_cost / combined_cost * 100) if combined_cost > 0 else 0\n",
    "    \n",
    "    database_cost = database_df['computedAmount'].sum() if len(database_df) > 0 else 0\n",
    "    database_pct = (database_cost / combined_cost * 100) if combined_cost > 0 else 0\n",
    "    \n",
    "    print(\"\\nüìä COMBINED SUMMARY:\")\n",
    "    print(\"-\" * 160)\n",
    "    print(f\"{'Total Exadata + Database Spend:':<40} ${combined_cost:>22,.2f}\")\n",
    "    print(f\"{'% of Total Cloud Spend:':<40} {combined_pct_total:>21.2f}%\")\n",
    "    print(f\"\\n{'Exadata Component:':<40} ${exadata_cost:>22,.2f} ({exadata_pct:>5.1f}%)\")\n",
    "    print(f\"{'Database Component:':<40} ${database_cost:>22,.2f} ({database_pct:>5.1f}%)\")\n",
    "    print(f\"{'Other Cloud Services:':<40} ${df['computedAmount'].sum() - combined_cost:>22,.2f} ({100 - combined_pct_total:>5.1f}%)\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # COST BREAKDOWN COMPARISON\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüîÑ COST BREAKDOWN BY CATEGORY:\")\n",
    "    print(\"-\" * 160)\n",
    "    \n",
    "    category_data = []\n",
    "    if len(exadata_df) > 0:\n",
    "        for service in exadata_df['service'].unique():\n",
    "            cost = exadata_df[exadata_df['service'] == service]['computedAmount'].sum()\n",
    "            category_data.append({'Category': f'Exadata: {service}', 'Cost': cost, 'Type': 'Exadata'})\n",
    "    \n",
    "    if len(database_df) > 0:\n",
    "        for service in database_df['service'].unique():\n",
    "            cost = database_df[database_df['service'] == service]['computedAmount'].sum()\n",
    "            category_data.append({'Category': f'Database: {service}', 'Cost': cost, 'Type': 'Database'})\n",
    "    \n",
    "    if category_data:\n",
    "        category_df = pd.DataFrame(category_data).sort_values('Cost', ascending=False)\n",
    "        print(f\"{'Category':<70} {'Cost':>18} {'% of Combined':>18}\")\n",
    "        print(\"-\" * 160)\n",
    "        for _, row in category_df.iterrows():\n",
    "            pct = (row['Cost'] / combined_cost) * 100\n",
    "            print(f\"{row['Category']:<70} ${row['Cost']:>17,.2f} {pct:>17.1f}%\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # TOP RESOURCES ACROSS BOTH (Top 30)\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüèÜ TOP 30 COMBINED EXADATA & DATABASE RESOURCES:\")\n",
    "    print(\"-\" * 160)\n",
    "    \n",
    "    all_resources = combined_df.groupby(['resourceId', 'resourceName', 'service', 'region_from_call2']).agg({\n",
    "        'computedAmount': 'sum',\n",
    "        'computedQuantity': 'sum'\n",
    "    }).reset_index().sort_values('computedAmount', ascending=False).head(30)\n",
    "    \n",
    "    print(f\"{'Rank':<6} {'Resource Name':<45} {'Type':<18} {'Service':<25} {'Cost':>18}\")\n",
    "    print(f\"{'':6} {'Region':<30} {'Daily Cost':>18}\")\n",
    "    print(\"-\" * 160)\n",
    "    \n",
    "    for idx, row in all_resources.iterrows():\n",
    "        rank = all_resources.index.get_loc(idx) + 1\n",
    "        resource_type = 'Exadata' if 'Exadata' in str(row['service']) else 'Database'\n",
    "        print(f\"{rank:<6} {str(row['resourceName'])[:43]:<45} {resource_type:<18} {str(row['service'])[:23]:<25} ${row['computedAmount']:>17,.2f}\")\n",
    "        print(f\"{'':6} {str(row['region_from_call2']):<30} ${row['computedAmount']/30:>17,.2f}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # MONTHLY TREND COMPARISON\n",
    "    # ========================================================================================\n",
    "    print(\"\\n\\nüìà MONTHLY COST TREND COMPARISON:\")\n",
    "    print(\"-\" * 160)\n",
    "    \n",
    "    combined_df['month'] = pd.to_datetime(combined_df['timeUsageStarted']).dt.to_period('M')\n",
    "    monthly_combined = combined_df.groupby('month').agg({\n",
    "        'computedAmount': 'sum'\n",
    "    }).reset_index()\n",
    "    monthly_combined['month'] = monthly_combined['month'].astype(str)\n",
    "    \n",
    "    if len(exadata_df) > 0:\n",
    "        exadata_df['month'] = pd.to_datetime(exadata_df['timeUsageStarted']).dt.to_period('M')\n",
    "        monthly_exadata = exadata_df.groupby('month').agg({\n",
    "            'computedAmount': 'sum'\n",
    "        }).reset_index()\n",
    "        monthly_exadata['month'] = monthly_exadata['month'].astype(str)\n",
    "        monthly_exadata.set_index('month', inplace=True)\n",
    "        monthly_exadata.rename(columns={'computedAmount': 'exadata_cost'}, inplace=True)\n",
    "    else:\n",
    "        monthly_exadata = pd.DataFrame()\n",
    "    \n",
    "    if len(database_df) > 0:\n",
    "        database_df['month'] = pd.to_datetime(database_df['timeUsageStarted']).dt.to_period('M')\n",
    "        monthly_database = database_df.groupby('month').agg({\n",
    "            'computedAmount': 'sum'\n",
    "        }).reset_index()\n",
    "        monthly_database['month'] = monthly_database['month'].astype(str)\n",
    "        monthly_database.set_index('month', inplace=True)\n",
    "        monthly_database.rename(columns={'computedAmount': 'database_cost'}, inplace=True)\n",
    "    else:\n",
    "        monthly_database = pd.DataFrame()\n",
    "    \n",
    "    monthly_combined.set_index('month', inplace=True)\n",
    "    monthly_combined.rename(columns={'computedAmount': 'total_cost'}, inplace=True)\n",
    "    \n",
    "    # Merge and display\n",
    "    if len(monthly_exadata) > 0 and len(monthly_database) > 0:\n",
    "        monthly_merge = monthly_combined.join([monthly_exadata, monthly_database], how='outer').fillna(0)\n",
    "        print(f\"{'Month':<12} {'Total Combined':>18} {'Exadata':>18} {'Database':>18} {'Exadata %':>12} {'DB %':>12}\")\n",
    "        print(\"-\" * 160)\n",
    "        for month, row in monthly_merge.iterrows():\n",
    "            ex_pct = (row['exadata_cost'] / row['total_cost'] * 100) if row['total_cost'] > 0 else 0\n",
    "            db_pct = (row['database_cost'] / row['total_cost'] * 100) if row['total_cost'] > 0 else 0\n",
    "            print(f\"{month:<12} ${row['total_cost']:>17,.2f} ${row['exadata_cost']:>17,.2f} ${row['database_cost']:>17,.2f} {ex_pct:>11.1f}% {db_pct:>11.1f}%\")\n",
    "    \n",
    "    # ========================================================================================\n",
    "    # VISUALIZATIONS\n",
    "    # ========================================================================================\n",
    "    fig = plt.figure(figsize=(22, 14))\n",
    "    gs = fig.add_gridspec(3, 2, hspace=0.35, wspace=0.3)\n",
    "    \n",
    "    # 1. Pie chart - Exadata vs Database\n",
    "    ax = fig.add_subplot(gs[0, 0])\n",
    "    sizes = [exadata_cost, database_cost]\n",
    "    labels = [f'Exadata\\n${exadata_cost:,.0f}', f'Database\\n${database_cost:,.0f}']\n",
    "    colors = ['steelblue', 'darkgreen']\n",
    "    ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    ax.set_title('Exadata vs Database Cost Share', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 2. Cost categories\n",
    "    ax = fig.add_subplot(gs[0, 1])\n",
    "    if category_data:\n",
    "        category_df_plot = pd.DataFrame(category_data).sort_values('Cost', ascending=True).tail(10)\n",
    "        colors_cat = ['steelblue' if x == 'Exadata' else 'darkgreen' for x in category_df_plot['Type']]\n",
    "        ax.barh(range(len(category_df_plot)), category_df_plot['Cost'], color=colors_cat)\n",
    "        ax.set_yticks(range(len(category_df_plot)))\n",
    "        ax.set_yticklabels([x[:40] for x in category_df_plot['Category']], fontsize=9)\n",
    "        ax.set_xlabel('Cost (USD)')\n",
    "        ax.set_title('Top 10 Service Categories', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 3. Top 15 resources\n",
    "    ax = fig.add_subplot(gs[1, :])\n",
    "    top_15 = all_resources.head(15).sort_values('computedAmount')\n",
    "    colors_res = ['steelblue' if 'Exadata' in x else 'darkgreen' for x in top_15['service']]\n",
    "    ax.barh(range(len(top_15)), top_15['computedAmount'], color=colors_res)\n",
    "    ax.set_yticks(range(len(top_15)))\n",
    "    ax.set_yticklabels([x[:45] for x in top_15['resourceName']], fontsize=9)\n",
    "    ax.set_xlabel('Cost (USD)')\n",
    "    ax.set_title('Top 15 Individual Resources (Exadata=Blue, Database=Green)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 4. Monthly trend\n",
    "    ax = fig.add_subplot(gs[2, :])\n",
    "    if len(monthly_exadata) > 0 and len(monthly_database) > 0:\n",
    "        monthly_merge_plot = monthly_merge.reset_index()\n",
    "        x_pos = range(len(monthly_merge_plot))\n",
    "        width = 0.35\n",
    "        ax.bar([p - width/2 for p in x_pos], monthly_merge_plot['exadata_cost'], width, label='Exadata', color='steelblue')\n",
    "        ax.bar([p + width/2 for p in x_pos], monthly_merge_plot['database_cost'], width, label='Database', color='darkgreen')\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(monthly_merge_plot['month'], rotation=45, ha='right')\n",
    "        ax.set_ylabel('Cost (USD)')\n",
    "        ax.set_title('Monthly Cost Trend: Exadata vs Database', fontsize=12, fontweight='bold')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Combined Analysis Complete\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No Exadata or Database infrastructure found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb159e",
   "metadata": {},
   "source": [
    "## Section 5: Export Analysis Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797817c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory if not exists\n",
    "output_dir = '../output/analysis_reports'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(\"=\"*160)\n",
    "print(\"EXADATA & DATABASE ANALYSIS - EXPORT REPORTS\")\n",
    "print(\"=\"*160)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. EXADATA DETAILED REPORT - CSV\n",
    "# ============================================================================\n",
    "print(\"1. Exporting Exadata Detailed Resource Report...\")\n",
    "try:\n",
    "    exadata_export = exadata_df.copy()\n",
    "    \n",
    "    # Select and reorder key columns\n",
    "    export_cols = ['date', 'service', 'resourceName', 'compartmentPath', 'region_from_call2', 'skuName', \n",
    "                   'computedQuantity', 'unitOfMeasure', 'computedAmount', 'lineStatus', 'timeUsageStarted']\n",
    "    available_cols = [col for col in export_cols if col in exadata_export.columns]\n",
    "    exadata_export = exadata_export[available_cols].sort_values('computedAmount', ascending=False)\n",
    "    \n",
    "    filename = f'{output_dir}/exadata_resources_{timestamp}.csv'\n",
    "    exadata_export.to_csv(filename, index=False)\n",
    "    print(f\"   ‚úì Saved: {filename} ({len(exadata_export)} rows)\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó Error: {str(e)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DATABASE DETAILED REPORT - CSV\n",
    "# ============================================================================\n",
    "print(\"2. Exporting Database Detailed Resource Report...\")\n",
    "try:\n",
    "    database_export = database_df.copy()\n",
    "    \n",
    "    export_cols = ['date', 'service', 'resourceName', 'compartmentPath', 'region_from_call2', 'skuName', \n",
    "                   'computedQuantity', 'unitOfMeasure', 'computedAmount', 'lineStatus', 'timeUsageStarted']\n",
    "    available_cols = [col for col in export_cols if col in database_export.columns]\n",
    "    database_export = database_export[available_cols].sort_values('computedAmount', ascending=False)\n",
    "    \n",
    "    filename = f'{output_dir}/database_resources_{timestamp}.csv'\n",
    "    database_export.to_csv(filename, index=False)\n",
    "    print(f\"   ‚úì Saved: {filename} ({len(database_export)} rows)\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó Error: {str(e)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. EXADATA EXECUTIVE SUMMARY - TEXT\n",
    "# ============================================================================\n",
    "print(\"3. Exporting Exadata Executive Summary Report...\")\n",
    "try:\n",
    "    with open(f'{output_dir}/exadata_summary_{timestamp}.txt', 'w') as f:\n",
    "        f.write(\"=\"*160 + \"\\n\")\n",
    "        f.write(\"EXADATA INFRASTRUCTURE - EXECUTIVE SUMMARY\\n\")\n",
    "        f.write(\"=\"*160 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"COST METRICS:\\n\")\n",
    "        f.write(\"-\" * 160 + \"\\n\")\n",
    "        total_cost_exadata = exadata_df['computedAmount'].sum()\n",
    "        total_cost_all = df['computedAmount'].sum()\n",
    "        pct_of_total = (total_cost_exadata / total_cost_all * 100) if total_cost_all > 0 else 0\n",
    "        \n",
    "        f.write(f\"  Total Exadata Cost:        ${total_cost_exadata:>15,.2f}\\n\")\n",
    "        f.write(f\"  Percentage of Total:       {pct_of_total:>15.2f}%\\n\")\n",
    "        f.write(f\"  Date Range:                {exadata_df['date'].min()} to {exadata_df['date'].max()}\\n\")\n",
    "        f.write(f\"  Total Usage Hours:         {exadata_df['computedQuantity'].sum():>15,.0f}\\n\")\n",
    "        f.write(f\"  Avg Hourly Cost:           ${(total_cost_exadata / exadata_df['computedQuantity'].sum() if exadata_df['computedQuantity'].sum() > 0 else 0):>15,.2f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"RESOURCE METRICS:\\n\")\n",
    "        f.write(\"-\" * 160 + \"\\n\")\n",
    "        f.write(f\"  Unique Systems:            {exadata_df['resourceName'].nunique():>15}\\n\")\n",
    "        f.write(f\"  Unique Services:           {exadata_df['service'].nunique():>15}\\n\")\n",
    "        f.write(f\"  Unique Regions:            {exadata_df['region_from_call2'].nunique():>15}\\n\")\n",
    "        f.write(f\"  Unique Compartments:       {exadata_df['compartmentPath'].nunique():>15}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PROJECTIONS:\\n\")\n",
    "        f.write(\"-\" * 160 + \"\\n\")\n",
    "        days_in_analysis = (exadata_df['date'].max() - exadata_df['date'].min()).days + 1\n",
    "        daily_avg = total_cost_exadata / max(days_in_analysis, 1)\n",
    "        f.write(f\"  Daily Average Cost:        ${daily_avg:>15,.2f}\\n\")\n",
    "        f.write(f\"  Monthly Projection:        ${daily_avg * 30:>15,.2f}\\n\")\n",
    "        f.write(f\"  Annual Projection:         ${daily_avg * 365:>15,.2f}\\n\\n\")\n",
    "        \n",
    "        # Top services\n",
    "        f.write(\"TOP SERVICES (by cost):\\n\")\n",
    "        f.write(\"-\" * 160 + \"\\n\")\n",
    "        top_services = exadata_df.groupby('service')['computedAmount'].sum().sort_values(ascending=False).head(10)\n",
    "        for idx, (service, cost) in enumerate(top_services.items(), 1):\n",
    "            pct = (cost / total_cost_exadata * 100) if total_cost_exadata > 0 else 0\n",
    "            f.write(f\"  {idx:2}. {service:<50} ${cost:>15,.2f} ({pct:>6.2f}%)\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"   ‚úì Saved: {output_dir}/exadata_summary_{timestamp}.txt\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó Error: {str(e)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. DATABASE EXECUTIVE SUMMARY - TEXT\n",
    "# ============================================================================\n",
    "print(\"4. Exporting Database Executive Summary Report...\")\n",
    "try:\n",
    "    with open(f'{output_dir}/database_summary_{timestamp}.txt', 'w') as f:\n",
    "        f.write(\"=\"*160 + \"\\n\")\n",
    "        f.write(\"DATABASE INFRASTRUCTURE - EXECUTIVE SUMMARY\\n\")\n",
    "        f.write(\"=\"*160 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"COST METRICS:\\n\")\n",
    "        f.write(\"-\" * 160 + \"\\n\")\n",
    "        total_cost_database = database_df['computedAmount'].sum()\n",
    "        pct_of_total = (total_cost_database / total_cost_all * 100) if total_cost_all > 0 else 0\n",
    "        \n",
    "        f.write(f\"  Total Database Cost:       ${total_cost_database:>15,.2f}\\n\")\n",
    "        f.write(f\"  Percentage of Total:       {pct_of_total:>15.2f}%\\n\")\n",
    "        f.write(f\"  Date Range:                {database_df['date'].min()} to {database_df['date'].max()}\\n\")\n",
    "        f.write(f\"  Total Usage Hours:         {database_df['computedQuantity'].sum():>15,.0f}\\n\")\n",
    "        f.write(f\"  Avg Hourly Cost:           ${(total_cost_database / database_df['computedQuantity'].sum() if database_df['computedQuantity'].sum() > 0 else 0):>15,.2f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"RESOURCE METRICS:\\n\")\n",
    "        f.write(\"-\" * 160 + \"\\n\")\n",
    "        f.write(f\"  Unique Instances:          {database_df['resourceName'].nunique():>15}\\n\")\n",
    "        f.write(f\"  Unique Services:           {database_df['service'].nunique():>15}\\n\")\n",
    "        f.write(f\"  Unique Regions:            {database_df['region_from_call2'].nunique():>15}\\n\")\n",
    "        f.write(f\"  Unique Compartments:       {database_df['compartmentPath'].nunique():>15}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PROJECTIONS:\\n\")\n",
    "        f.write(\"-\" * 160 + \"\\n\")\n",
    "        days_in_analysis = (database_df['date'].max() - database_df['date'].min()).days + 1\n",
    "        daily_avg = total_cost_database / max(days_in_analysis, 1)\n",
    "        f.write(f\"  Daily Average Cost:        ${daily_avg:>15,.2f}\\n\")\n",
    "        f.write(f\"  Monthly Projection:        ${daily_avg * 30:>15,.2f}\\n\")\n",
    "        f.write(f\"  Annual Projection:         ${daily_avg * 365:>15,.2f}\\n\\n\")\n",
    "        \n",
    "        # Top services\n",
    "        f.write(\"TOP SERVICES (by cost):\\n\")\n",
    "        f.write(\"-\" * 160 + \"\\n\")\n",
    "        top_services = database_df.groupby('service')['computedAmount'].sum().sort_values(ascending=False).head(10)\n",
    "        for idx, (service, cost) in enumerate(top_services.items(), 1):\n",
    "            pct = (cost / total_cost_database * 100) if total_cost_database > 0 else 0\n",
    "            f.write(f\"  {idx:2}. {service:<50} ${cost:>15,.2f} ({pct:>6.2f}%)\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"   ‚úì Saved: {output_dir}/database_summary_{timestamp}.txt\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó Error: {str(e)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. COMBINED COST COMPARISON - CSV\n",
    "# ============================================================================\n",
    "print(\"5. Exporting Combined Cost Comparison (Daily Aggregation)...\")\n",
    "try:\n",
    "    # Get daily costs for each category\n",
    "    exadata_daily = exadata_df.groupby('date')[['computedAmount', 'computedQuantity']].sum().reset_index()\n",
    "    exadata_daily.columns = ['date', 'exadata_cost', 'exadata_hours']\n",
    "    \n",
    "    database_daily = database_df.groupby('date')[['computedAmount', 'computedQuantity']].sum().reset_index()\n",
    "    database_daily.columns = ['date', 'database_cost', 'database_hours']\n",
    "    \n",
    "    # Merge and create combined view\n",
    "    combined_daily = exadata_daily.merge(database_daily, on='date', how='outer').fillna(0)\n",
    "    combined_daily['total_cost'] = combined_daily['exadata_cost'] + combined_daily['database_cost']\n",
    "    combined_daily['exadata_pct'] = (combined_daily['exadata_cost'] / combined_daily['total_cost'] * 100).replace([np.inf, -np.inf], 0)\n",
    "    combined_daily['database_pct'] = (combined_daily['database_cost'] / combined_daily['total_cost'] * 100).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    filename = f'{output_dir}/daily_comparison_{timestamp}.csv'\n",
    "    combined_daily.to_csv(filename, index=False)\n",
    "    print(f\"   ‚úì Saved: {filename} ({len(combined_daily)} rows)\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó Error: {str(e)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 6. TOP 50 RESOURCES - COMBINED CSV\n",
    "# ============================================================================\n",
    "print(\"6. Exporting Top 50 Combined Resources...\")\n",
    "try:\n",
    "    # Create combined resource view\n",
    "    exadata_resources = exadata_df.groupby(['resourceName', 'service', 'compartmentPath', 'region_from_call2']).agg(\n",
    "        cost=('computedAmount', 'sum'),\n",
    "        hours=('computedQuantity', 'sum'),\n",
    "        type=('service', 'first')\n",
    "    ).reset_index()\n",
    "    exadata_resources['category'] = 'Exadata'\n",
    "    \n",
    "    database_resources = database_df.groupby(['resourceName', 'service', 'compartmentPath', 'region_from_call2']).agg(\n",
    "        cost=('computedAmount', 'sum'),\n",
    "        hours=('computedQuantity', 'sum'),\n",
    "        type=('service', 'first')\n",
    "    ).reset_index()\n",
    "    database_resources['category'] = 'Database'\n",
    "    \n",
    "    # Combine and sort\n",
    "    top_resources = pd.concat([exadata_resources, database_resources], ignore_index=True)\n",
    "    top_resources = top_resources.sort_values('cost', ascending=False).head(50)\n",
    "    top_resources['rank'] = range(1, len(top_resources) + 1)\n",
    "    days_in_analysis = (combined_df['date'].max() - combined_df['date'].min()).days + 1\n",
    "    top_resources['daily_cost'] = top_resources['cost'] / max(days_in_analysis, 1)\n",
    "    \n",
    "    # Reorder columns\n",
    "    top_resources = top_resources[['rank', 'category', 'resourceName', 'service', 'compartmentPath', \n",
    "                                    'region_from_call2', 'cost', 'daily_cost', 'hours']]\n",
    "    \n",
    "    filename = f'{output_dir}/top_50_resources_{timestamp}.csv'\n",
    "    top_resources.to_csv(filename, index=False)\n",
    "    print(f\"   ‚úì Saved: {filename} ({len(top_resources)} rows)\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó Error: {str(e)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 7. COST ALLOCATION BY COMPARTMENT - CSV\n",
    "# ============================================================================\n",
    "print(\"7. Exporting Cost Allocation by Compartment...\")\n",
    "try:\n",
    "    exadata_compartment = exadata_df.groupby('compartmentPath').agg(\n",
    "        cost=('computedAmount', 'sum'),\n",
    "        resources=('resourceName', 'nunique'),\n",
    "        services=('service', 'nunique')\n",
    "    ).reset_index()\n",
    "    exadata_compartment['category'] = 'Exadata'\n",
    "    exadata_compartment = exadata_compartment.sort_values('cost', ascending=False)\n",
    "    \n",
    "    database_compartment = database_df.groupby('compartmentPath').agg(\n",
    "        cost=('computedAmount', 'sum'),\n",
    "        resources=('resourceName', 'nunique'),\n",
    "        services=('service', 'nunique')\n",
    "    ).reset_index()\n",
    "    database_compartment['category'] = 'Database'\n",
    "    database_compartment = database_compartment.sort_values('cost', ascending=False)\n",
    "    \n",
    "    # Combine\n",
    "    compartment_allocation = pd.concat([exadata_compartment, database_compartment], ignore_index=True)\n",
    "    \n",
    "    filename = f'{output_dir}/cost_by_compartment_{timestamp}.csv'\n",
    "    compartment_allocation.to_csv(filename, index=False)\n",
    "    print(f\"   ‚úì Saved: {filename} ({len(compartment_allocation)} rows)\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó Error: {str(e)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 8. COST ALLOCATION BY REGION - CSV\n",
    "# ============================================================================\n",
    "print(\"8. Exporting Cost Allocation by Region...\")\n",
    "try:\n",
    "    exadata_region = exadata_df.groupby('region_from_call2').agg(\n",
    "        cost=('computedAmount', 'sum'),\n",
    "        resources=('resourceName', 'nunique'),\n",
    "        services=('service', 'nunique')\n",
    "    ).reset_index()\n",
    "    exadata_region['category'] = 'Exadata'\n",
    "    exadata_region = exadata_region.sort_values('cost', ascending=False)\n",
    "    \n",
    "    database_region = database_df.groupby('region_from_call2').agg(\n",
    "        cost=('computedAmount', 'sum'),\n",
    "        resources=('resourceName', 'nunique'),\n",
    "        services=('service', 'nunique')\n",
    "    ).reset_index()\n",
    "    database_region['category'] = 'Database'\n",
    "    database_region = database_region.sort_values('cost', ascending=False)\n",
    "    \n",
    "    # Combine\n",
    "    region_allocation = pd.concat([exadata_region, database_region], ignore_index=True)\n",
    "    \n",
    "    filename = f'{output_dir}/cost_by_region_{timestamp}.csv'\n",
    "    region_allocation.to_csv(filename, index=False)\n",
    "    print(f\"   ‚úì Saved: {filename} ({len(region_allocation)} rows)\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó Error: {str(e)}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*160)\n",
    "print(\"EXPORT COMPLETE\")\n",
    "print(\"=\"*160)\n",
    "print(f\"Output Directory: {output_dir}/\")\n",
    "print(f\"Report Files Generated:\")\n",
    "print(f\"  - Exadata Resources CSV\")\n",
    "print(f\"  - Database Resources CSV\")\n",
    "print(f\"  - Exadata Summary Text Report\")\n",
    "print(f\"  - Database Summary Text Report\")\n",
    "print(f\"  - Daily Comparison CSV\")\n",
    "print(f\"  - Top 50 Combined Resources CSV\")\n",
    "print(f\"  - Cost by Compartment CSV\")\n",
    "print(f\"  - Cost by Region CSV\")\n",
    "print(\"=\"*160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f9bcf1",
   "metadata": {},
   "source": [
    "## Section 6: Advanced Analytics - Forecasting & Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61131a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*160)\n",
    "print(\"ADVANCED ANALYTICS - FORECASTING & ANOMALY DETECTION\")\n",
    "print(\"=\"*160)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. EXADATA COST FORECAST (7-Day Linear Projection)\n",
    "# ============================================================================\n",
    "print(\"1. EXADATA COST FORECAST (7-Day Projection using Linear Regression)\")\n",
    "print(\"-\"*160)\n",
    "\n",
    "try:\n",
    "    # Prepare data for Exadata forecast\n",
    "    exadata_daily = exadata_df.groupby('date')['computedAmount'].sum().reset_index()\n",
    "    exadata_daily = exadata_daily.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    if len(exadata_daily) >= 3:\n",
    "        # Create numerical X values (days from start)\n",
    "        X = np.arange(len(exadata_daily)).reshape(-1, 1)\n",
    "        y = exadata_daily['computedAmount'].values\n",
    "        \n",
    "        # Fit linear regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Generate forecast for next 7 days\n",
    "        last_date = exadata_daily['date'].max()\n",
    "        forecast_dates = [last_date + timedelta(days=i+1) for i in range(7)]\n",
    "        forecast_X = np.arange(len(exadata_daily), len(exadata_daily) + 7).reshape(-1, 1)\n",
    "        forecast_y = model.predict(forecast_X)\n",
    "        \n",
    "        forecast_df = pd.DataFrame({\n",
    "            'date': forecast_dates,\n",
    "            'forecasted_cost': forecast_y,\n",
    "            'trend': 'forecast'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nExadata Daily Cost Forecast (Next 7 Days):\")\n",
    "        print(\"-\"*160)\n",
    "        print(f\"{'Date':<15} {'Forecasted Cost':>20} {'vs Previous Day':>20}\")\n",
    "        print(\"-\"*160)\n",
    "        \n",
    "        prev_cost = exadata_daily['computedAmount'].iloc[-1]\n",
    "        for idx, row in forecast_df.iterrows():\n",
    "            diff = row['forecasted_cost'] - prev_cost\n",
    "            diff_pct = (diff / prev_cost * 100) if prev_cost > 0 else 0\n",
    "            print(f\"{str(row['date'].date()):<15} ${row['forecasted_cost']:>18,.2f} {diff_pct:>18.2f}%\")\n",
    "            prev_cost = row['forecasted_cost']\n",
    "        \n",
    "        # Weekly summary\n",
    "        print(\"\\n\" + \"-\"*160)\n",
    "        print(f\"Current Daily Average:     ${exadata_daily['computedAmount'].mean():>18,.2f}\")\n",
    "        print(f\"Forecast Daily Average:    ${forecast_df['forecasted_cost'].mean():>18,.2f}\")\n",
    "        print(f\"7-Day Forecast Total:      ${forecast_df['forecasted_cost'].sum():>18,.2f}\")\n",
    "        print(f\"Trend Direction:           {'INCREASING' if model.coef_[0] > 0 else 'DECREASING'} (${model.coef_[0]:.2f}/day)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Insufficient data for forecast (need at least 3 days)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in Exadata forecast: {str(e)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DATABASE COST FORECAST (7-Day Linear Projection)\n",
    "# ============================================================================\n",
    "print(\"2. DATABASE COST FORECAST (7-Day Projection using Linear Regression)\")\n",
    "print(\"-\"*160)\n",
    "\n",
    "try:\n",
    "    # Prepare data for Database forecast\n",
    "    database_daily = database_df.groupby('date')['computedAmount'].sum().reset_index()\n",
    "    database_daily = database_daily.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    if len(database_daily) >= 3:\n",
    "        # Create numerical X values\n",
    "        X = np.arange(len(database_daily)).reshape(-1, 1)\n",
    "        y = database_daily['computedAmount'].values\n",
    "        \n",
    "        # Fit linear regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Generate forecast\n",
    "        last_date = database_daily['date'].max()\n",
    "        forecast_dates = [last_date + timedelta(days=i+1) for i in range(7)]\n",
    "        forecast_X = np.arange(len(database_daily), len(database_daily) + 7).reshape(-1, 1)\n",
    "        forecast_y = model.predict(forecast_X)\n",
    "        \n",
    "        forecast_df = pd.DataFrame({\n",
    "            'date': forecast_dates,\n",
    "            'forecasted_cost': forecast_y\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nDatabase Daily Cost Forecast (Next 7 Days):\")\n",
    "        print(\"-\"*160)\n",
    "        print(f\"{'Date':<15} {'Forecasted Cost':>20} {'vs Previous Day':>20}\")\n",
    "        print(\"-\"*160)\n",
    "        \n",
    "        prev_cost = database_daily['computedAmount'].iloc[-1]\n",
    "        for idx, row in forecast_df.iterrows():\n",
    "            diff = row['forecasted_cost'] - prev_cost\n",
    "            diff_pct = (diff / prev_cost * 100) if prev_cost > 0 else 0\n",
    "            print(f\"{str(row['date'].date()):<15} ${row['forecasted_cost']:>18,.2f} {diff_pct:>18.2f}%\")\n",
    "            prev_cost = row['forecasted_cost']\n",
    "        \n",
    "        # Weekly summary\n",
    "        print(\"\\n\" + \"-\"*160)\n",
    "        print(f\"Current Daily Average:     ${database_daily['computedAmount'].mean():>18,.2f}\")\n",
    "        print(f\"Forecast Daily Average:    ${forecast_df['forecasted_cost'].mean():>18,.2f}\")\n",
    "        print(f\"7-Day Forecast Total:      ${forecast_df['forecasted_cost'].sum():>18,.2f}\")\n",
    "        print(f\"Trend Direction:           {'INCREASING' if model.coef_[0] > 0 else 'DECREASING'} (${model.coef_[0]:.2f}/day)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Insufficient data for forecast (need at least 3 days)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in Database forecast: {str(e)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ANOMALY DETECTION - COST SPIKES (Exadata)\n",
    "# ============================================================================\n",
    "print(\"3. ANOMALY DETECTION - EXADATA COST SPIKES\")\n",
    "print(\"-\"*160)\n",
    "\n",
    "try:\n",
    "    exadata_daily = exadata_df.groupby('date')['computedAmount'].sum().reset_index()\n",
    "    exadata_daily = exadata_daily.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    if len(exadata_daily) >= 5:\n",
    "        # Calculate mean and standard deviation\n",
    "        mean_cost = exadata_daily['computedAmount'].mean()\n",
    "        std_cost = exadata_daily['computedAmount'].std()\n",
    "        threshold = mean_cost + (2 * std_cost)  # 2 std dev threshold\n",
    "        \n",
    "        # Identify anomalies\n",
    "        anomalies = exadata_daily[exadata_daily['computedAmount'] > threshold].sort_values('computedAmount', ascending=False)\n",
    "        \n",
    "        print(f\"\\nExadata Cost Anomalies (>2 std dev above mean):\")\n",
    "        print(\"-\"*160)\n",
    "        print(f\"Mean Daily Cost:           ${mean_cost:>18,.2f}\")\n",
    "        print(f\"Std Deviation:             ${std_cost:>18,.2f}\")\n",
    "        print(f\"Anomaly Threshold:         ${threshold:>18,.2f}\")\n",
    "        print()\n",
    "        \n",
    "        if len(anomalies) > 0:\n",
    "            print(f\"Detected {len(anomalies)} anomalous days:\")\n",
    "            print(f\"{'Date':<15} {'Cost':>20} {'Deviation':>20} {'Type':>30}\")\n",
    "            print(\"-\"*160)\n",
    "            for idx, row in anomalies.iterrows():\n",
    "                deviation = ((row['computedAmount'] - mean_cost) / std_cost)\n",
    "                print(f\"{str(row['date'].date()):<15} ${row['computedAmount']:>18,.2f} {deviation:>19.2f}œÉ {'SPIKE' if row['computedAmount'] > mean_cost else 'DIP':<30}\")\n",
    "        else:\n",
    "            print(\"No significant anomalies detected (all costs within 2 std dev)\")\n",
    "    else:\n",
    "        print(\"Insufficient data for anomaly detection (need at least 5 days)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in anomaly detection: {str(e)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. ANOMALY DETECTION - COST SPIKES (Database)\n",
    "# ============================================================================\n",
    "print(\"4. ANOMALY DETECTION - DATABASE COST SPIKES\")\n",
    "print(\"-\"*160)\n",
    "\n",
    "try:\n",
    "    database_daily = database_df.groupby('date')['computedAmount'].sum().reset_index()\n",
    "    database_daily = database_daily.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    if len(database_daily) >= 5:\n",
    "        # Calculate mean and standard deviation\n",
    "        mean_cost = database_daily['computedAmount'].mean()\n",
    "        std_cost = database_daily['computedAmount'].std()\n",
    "        threshold = mean_cost + (2 * std_cost)  # 2 std dev threshold\n",
    "        \n",
    "        # Identify anomalies\n",
    "        anomalies = database_daily[database_daily['computedAmount'] > threshold].sort_values('computedAmount', ascending=False)\n",
    "        \n",
    "        print(f\"\\nDatabase Cost Anomalies (>2 std dev above mean):\")\n",
    "        print(\"-\"*160)\n",
    "        print(f\"Mean Daily Cost:           ${mean_cost:>18,.2f}\")\n",
    "        print(f\"Std Deviation:             ${std_cost:>18,.2f}\")\n",
    "        print(f\"Anomaly Threshold:         ${threshold:>18,.2f}\")\n",
    "        print()\n",
    "        \n",
    "        if len(anomalies) > 0:\n",
    "            print(f\"Detected {len(anomalies)} anomalous days:\")\n",
    "            print(f\"{'Date':<15} {'Cost':>20} {'Deviation':>20} {'Type':>30}\")\n",
    "            print(\"-\"*160)\n",
    "            for idx, row in anomalies.iterrows():\n",
    "                deviation = ((row['computedAmount'] - mean_cost) / std_cost)\n",
    "                print(f\"{str(row['date'].date()):<15} ${row['computedAmount']:>18,.2f} {deviation:>19.2f}œÉ {'SPIKE' if row['computedAmount'] > mean_cost else 'DIP':<30}\")\n",
    "        else:\n",
    "            print(\"No significant anomalies detected (all costs within 2 std dev)\")\n",
    "    else:\n",
    "        print(\"Insufficient data for anomaly detection (need at least 5 days)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in anomaly detection: {str(e)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TOP COST INCREASES - RESOURCES WITH GROWING COSTS\n",
    "# ============================================================================\n",
    "print(\"5. RESOURCES WITH MOST SIGNIFICANT COST INCREASES\")\n",
    "print(\"-\"*160)\n",
    "\n",
    "try:\n",
    "    # Split dataset into two halves and compare\n",
    "    exadata_sorted = exadata_df.sort_values('date')\n",
    "    midpoint = len(exadata_sorted) // 2\n",
    "    \n",
    "    if midpoint > 0:\n",
    "        first_half = exadata_sorted.iloc[:midpoint]\n",
    "        second_half = exadata_sorted.iloc[midpoint:]\n",
    "        \n",
    "        first_half_costs = first_half.groupby('resourceName')['computedAmount'].sum()\n",
    "        second_half_costs = second_half.groupby('resourceName')['computedAmount'].sum()\n",
    "        \n",
    "        # Calculate changes\n",
    "        all_resources = set(first_half_costs.index) | set(second_half_costs.index)\n",
    "        cost_changes = []\n",
    "        \n",
    "        for resource in all_resources:\n",
    "            first_cost = first_half_costs.get(resource, 0)\n",
    "            second_cost = second_half_costs.get(resource, 0)\n",
    "            change = second_cost - first_cost\n",
    "            change_pct = (change / first_cost * 100) if first_cost > 0 else (100 if second_cost > 0 else 0)\n",
    "            \n",
    "            cost_changes.append({\n",
    "                'resource': resource,\n",
    "                'first_half_cost': first_cost,\n",
    "                'second_half_cost': second_cost,\n",
    "                'change': change,\n",
    "                'change_pct': change_pct\n",
    "            })\n",
    "        \n",
    "        changes_df = pd.DataFrame(cost_changes).sort_values('change_pct', ascending=False)\n",
    "        \n",
    "        print(\"\\nExadata Resources with Largest Cost Increases:\")\n",
    "        print(f\"{'Rank':<5} {'Resource':<40} {'Change ($)':>15} {'Change (%)':>15}\")\n",
    "        print(\"-\"*160)\n",
    "        \n",
    "        for idx, row in changes_df[changes_df['change'] > 0].head(10).iterrows():\n",
    "            rank = changes_df.index.tolist().index(idx) + 1\n",
    "            resource_name = row['resource'][:38] if len(row['resource']) > 38 else row['resource']\n",
    "            print(f\"{rank:<5} {resource_name:<40} ${row['change']:>13,.2f} {row['change_pct']:>14.1f}%\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*160)\n",
    "        print(\"Analysis complete. Review trends for cost optimization opportunities.\")\n",
    "        print(\"=\"*160)\n",
    "    else:\n",
    "        print(\"Insufficient data for trend analysis\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in cost increase analysis: {str(e)}\")\n",
    "\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
